{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":952451,"sourceType":"datasetVersion","datasetId":517218},{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458},{"sourceId":8857973,"sourceType":"datasetVersion","datasetId":5223027},{"sourceId":14083871,"sourceType":"datasetVersion","datasetId":8966663}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# @title 1Ô∏è‚É£ SETUP: Clone Project & Submodules\nimport os\nimport shutil\nimport subprocess\nimport torch\n\n# --- C·∫§U H√åNH ---\nREPO_URL = \"https://github.com/rishubhpar/Depth-Aware-Editing.git\"\nTEMP_DIR = \"/kaggle/temp/Depth-Aware-Editing\"\nWORK_LINK = \"/kaggle/working/Depth-Aware-Editing\"\n\nprint(\"üöÄ B·∫ÆT ƒê·∫¶U KH·ªûI T·∫†O (Mode: Stable Diffusion 1.5)...\")\n\n# 1. Ki·ªÉm tra GPU\nif not torch.cuda.is_available():\n    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a b·∫≠t GPU! V√†o Session Options -> Accelerator -> GPU T4 x2.\")\nelse:\n    print(f\"‚úÖ GPU OK: {torch.cuda.get_device_name(0)}\")\n\ndef run_cmd(cmd):\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL)\n\n# 2. D·ªçn d·∫πp & Clone\nif os.path.exists(\"/kaggle/working\"): os.chdir(\"/kaggle/working\")\nif os.path.exists(WORK_LINK):\n    if os.path.islink(WORK_LINK): os.unlink(WORK_LINK)\n    else: shutil.rmtree(WORK_LINK)\nif os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)\n\nprint(\"‚¨áÔ∏è ƒêang clone Repository ch√≠nh...\")\nrun_cmd(f\"git clone {REPO_URL} {TEMP_DIR}\")\nos.symlink(TEMP_DIR, WORK_LINK)\nos.chdir(TEMP_DIR)\n\n# 3. Clone Submodules Th·ªß c√¥ng\nprint(\"üèóÔ∏è ƒêang clone c√°c Module con (SAM & GroundingDINO)...\")\nrun_cmd(\"git clone https://github.com/IDEA-Research/Grounded-Segment-Anything.git src/grounded_sam\")\nif os.path.exists(\"src/grounded_sam/GroundingDINO\"): shutil.rmtree(\"src/grounded_sam/GroundingDINO\")\nrun_cmd(\"git clone https://github.com/IDEA-Research/GroundingDINO.git src/grounded_sam/GroundingDINO\")\n\nprint(\"‚úÖ B∆Ø·ªöC 1 HO√ÄN T·∫§T.\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:13:01.910696Z","iopub.execute_input":"2025-12-25T01:13:01.910932Z","iopub.status.idle":"2025-12-25T01:13:13.495027Z","shell.execute_reply.started":"2025-12-25T01:13:01.910907Z","shell.execute_reply":"2025-12-25T01:13:13.494370Z"}},"outputs":[{"name":"stdout","text":"üöÄ B·∫ÆT ƒê·∫¶U KH·ªûI T·∫†O (Mode: Stable Diffusion 1.5)...\n‚úÖ GPU OK: Tesla T4\n‚¨áÔ∏è ƒêang clone Repository ch√≠nh...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/temp/Depth-Aware-Editing'...\n","output_type":"stream"},{"name":"stdout","text":"üèóÔ∏è ƒêang clone c√°c Module con (SAM & GroundingDINO)...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into 'src/grounded_sam'...\nCloning into 'src/grounded_sam/GroundingDINO'...\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ B∆Ø·ªöC 1 HO√ÄN T·∫§T.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# @title 2Ô∏è‚É£ INSTALL: C√†i ƒë·∫∑t Th∆∞ vi·ªán (FORCE OVERRIDE VERSION)\nimport os\nimport subprocess\nimport sys\n\n# ƒê·∫£m b·∫£o l√†m vi·ªác t·∫°i th∆∞ m·ª•c temp\nos.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n\nprint(\"üì¶ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán (Ch·∫ø ƒë·ªô C∆∞·ª°ng b·ª©c Version)...\")\nprint(\"‚ö†Ô∏è Code n√†y s·∫Ω g·ª° v√† c√†i l·∫°i nhi·ªÅu l·∫ßn ƒë·ªÉ √©p ƒë√∫ng version. H√£y ki√™n nh·∫´n!\\n\")\n\ndef run_cmd(cmd, desc=\"\"):\n    if desc:\n        print(f\"    ‚¨áÔ∏è {desc}...\")\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# ============================================================\n# B∆Ø·ªöC 1: D·ªåN D·∫∏P S·∫†CH S·∫º\n# ============================================================\nprint(\"üßπ B∆∞·ªõc 1/6: G·ª° b·ªè c√°c th∆∞ vi·ªán g√¢y xung ƒë·ªôt...\")\nuninstall_pkgs = [\n    \"numpy\", \"scipy\", \"scikit-learn\", \"matplotlib\", \"torchmetrics\",\n    \"torch\", \"torchvision\", \"torchaudio\", \"xformers\", \n    \"pytorch-lightning\", \"lightning\",\n    \"transformers\", \"diffusers\", \"accelerate\", \"huggingface_hub\", \"peft\",\n    \"gradio\", \"opencv-python\", \"opencv-python-headless\", \"albumentations\",\n    \"segment-anything\", \"groundingdino\", \n    \"tensorflow\", \"keras\", \"tensorboard\", \n    \"jax\", \"jaxlib\"\n]\nrun_cmd(f\"pip uninstall -y {' '.join(uninstall_pkgs)}\")\n\n# ============================================================\n# B∆Ø·ªöC 2: C√ÄI PYTORCH & CUDA\n# ============================================================\nprint(\"üî• B∆∞·ªõc 2/6: C√†i ƒë·∫∑t PyTorch 2.1...\")\nrun_cmd(\n    \"pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \"\n    \"--index-url https://download.pytorch.org/whl/cu121\"\n)\nrun_cmd(\n    \"pip install xformers==0.0.23 post1 --index-url https://download.pytorch.org/whl/cu121\"\n)\n\n# ============================================================\n# B∆Ø·ªöC 3: C√ÄI C√ÅC TH∆Ø VI·ªÜN N·ªÄN T·∫¢NG (Transformers, Accelerate...)\n# ============================================================\nprint(\"ü§ñ B∆∞·ªõc 3/6: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán n·ªÅn (Cho ph√©p c√†i Hub m·ªõi t·∫°m th·ªùi)...\")\n# ·ªû b∆∞·ªõc n√†y, ta c·ª© ƒë·ªÉ n√≥ c√†i b·∫£n m·ªõi nh·∫•t n·∫øu n√≥ mu·ªën, ta s·∫Ω s·ª≠a sau\nai_libs = [\n    \"opencv-python-headless\", \"albumentations\", \"imageio\", \"moviepy\",\n    \"transformers==4.39.3\", \"accelerate==0.28.0\",\n    \"safetensors\", \"omegaconf\", \"einops\", \"timm\",\n    \"addict\", \"yapf\", \"pyyaml\", \"gradio==3.50.2\", \"protobuf==3.20.3\",\n    \"scikit-learn\", \"matplotlib\", \n    \"pytorch-lightning==2.0.9\", \"torchmetrics==1.2.1\"\n]\nrun_cmd(f\"pip install {' '.join(ai_libs)}\")\n\n# ============================================================\n# B∆Ø·ªöC 4: C√ÄI MODULES NGO√ÄI (SAM & DINO)\n# ============================================================\nprint(\"üéØ B∆∞·ªõc 4/6: C√†i ƒë·∫∑t SAM & GroundingDINO...\")\nrun_cmd(\"pip install git+https://github.com/facebookresearch/segment-anything.git\")\nrun_cmd(\"pip install git+https://github.com/IDEA-Research/GroundingDINO.git\")\n\n# ============================================================\n# B∆Ø·ªöC 5: C∆Ø·ª†NG B·ª®C C√ÄI ƒê·∫∂T C√ÅC VERSION QUAN TR·ªåNG (THE FIX)\n# ============================================================\nprint(\"üî® B∆∞·ªõc 5/6: C∆∞·ª°ng b·ª©c c√†i ƒë·∫∑t Diffusers, PEFT v√† Hub c≈©...\")\n\n# 1. G·ª° b·ªè b·∫£n Hub m·ªõi (0.36.0) m√† c√°c th∆∞ vi·ªán tr√™n v·ª´a l√©n c√†i v√†o\nrun_cmd(\"pip uninstall -y huggingface_hub diffusers peft\")\n\n# 2. C√†i ƒë·∫∑t l·∫°i v·ªõi c·ªù --no-deps (Kh√¥ng c√†i dependencies ph·ª• thu·ªôc ƒë·ªÉ tr√°nh update ng∆∞·ª£c l·∫°i)\n# ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t: √âp bu·ªôc d√πng ƒë√∫ng version n√†y.\ninstall_force = [\n    \"huggingface_hub<0.25.0\",  # Fix l·ªói cached_download\n    \"diffusers==0.27.2\",       # Fix l·ªói GLIGEN\n    \"peft==0.8.2\"              # Fix l·ªói t∆∞∆°ng th√≠ch\n]\nfor pkg in install_force:\n    print(f\"      üëâ √âp c√†i: {pkg}\")\n    run_cmd(f\"pip install '{pkg}' --force-reinstall --no-deps\")\n\n# ============================================================\n# B∆Ø·ªöC 6: CH·ªêT H·∫† VERSION NUMPY & SCIPY\n# ============================================================\nprint(\"üî® B∆∞·ªõc 6/7: √âp phi√™n b·∫£n NumPy 1.26.4...\")\nrun_cmd(\"pip uninstall -y jax jaxlib numpy scipy\")\nrun_cmd(\"pip install 'numpy==1.26.4' 'scipy==1.11.4' --force-reinstall\")\n\n# TH√äM: Fix numpy.dtype binary incompatibility\nprint(\"üîß B∆∞·ªõc 7/7: Recompile packages ph·ª• thu·ªôc numpy...\")\nrun_cmd(\"pip install --force-reinstall --no-cache-dir scikit-learn matplotlib pillow opencv-python-headless\")\n\n# ============================================================\n# KI·ªÇM TRA K·∫æT QU·∫¢\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîç KI·ªÇM TRA PHI√äN B·∫¢N (FINAL CHECK):\")\nprint(\"=\"*60)\n\ncheck_cmds = [\n    (\"NumPy (Target: 1.26.4)\", \"import numpy; print(numpy.__version__)\"),\n    (\"Diffusers (Target: 0.27.2)\", \"import diffusers; print(diffusers.__version__)\"),\n    (\"HuggingFace Hub (Target: <0.25)\", \"import huggingface_hub; print(huggingface_hub.__version__)\"),\n    (\"PEFT (Target: 0.8.2)\", \"import peft; print(peft.__version__)\")\n]\n\nfor name, cmd in check_cmds:\n    result = subprocess.run([sys.executable, \"-c\", cmd], capture_output=True, text=True)\n    ver = result.stdout.strip() if result.returncode == 0 else \"ERROR\"\n    \n    # Logic ki·ªÉm tra m√†u\n    status = \"‚úÖ\"\n    if \"ERROR\" in ver: \n        status = \"‚ùå\"\n    elif name.startswith(\"HuggingFace\") and ver.startswith(\"0.3\"): # N·∫øu Hub l·∫°i nh·∫£y l√™n 0.3x\n        status = \"‚ùå (V·∫´n b·ªã ghi ƒë√®!)\"\n    \n    print(f\"    {status} {name}: {ver}\")\n\nprint(\"=\"*60)\nprint(\"‚úÖ QU√Å TR√åNH C√ÄI ƒê·∫∂T HO√ÄN T·∫§T!\")\n\nprint(\"\\n‚ö†Ô∏è  B∆Ø·ªöC QUY·∫æT ƒê·ªäNH: RESTART SESSION NGAY.\")\nprint(\"=\"*60)\n\nprint(\"    üëâ Ch·ªçn Menu: Run -> Restart Session\")\nprint(\"    üëâ Sau khi Restart: Ch·∫°y ti·∫øp Cell 3 (ƒë√£ s·ª≠a ·ªü tr√™n), r·ªìi Cell 4, 5.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:13:13.496523Z","iopub.execute_input":"2025-12-25T01:13:13.496940Z","iopub.status.idle":"2025-12-25T01:19:03.097106Z","shell.execute_reply.started":"2025-12-25T01:13:13.496914Z","shell.execute_reply":"2025-12-25T01:19:03.096288Z"}},"outputs":[{"name":"stdout","text":"üì¶ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán (Ch·∫ø ƒë·ªô C∆∞·ª°ng b·ª©c Version)...\n‚ö†Ô∏è Code n√†y s·∫Ω g·ª° v√† c√†i l·∫°i nhi·ªÅu l·∫ßn ƒë·ªÉ √©p ƒë√∫ng version. H√£y ki√™n nh·∫´n!\n\nüßπ B∆∞·ªõc 1/6: G·ª° b·ªè c√°c th∆∞ vi·ªán g√¢y xung ƒë·ªôt...\nüî• B∆∞·ªõc 2/6: C√†i ƒë·∫∑t PyTorch 2.1...\nü§ñ B∆∞·ªõc 3/6: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán n·ªÅn (Cho ph√©p c√†i Hub m·ªõi t·∫°m th·ªùi)...\nüéØ B∆∞·ªõc 4/6: C√†i ƒë·∫∑t SAM & GroundingDINO...\nüî® B∆∞·ªõc 5/6: C∆∞·ª°ng b·ª©c c√†i ƒë·∫∑t Diffusers, PEFT v√† Hub c≈©...\n      üëâ √âp c√†i: huggingface_hub<0.25.0\n      üëâ √âp c√†i: diffusers==0.27.2\n      üëâ √âp c√†i: peft==0.8.2\nüî® B∆∞·ªõc 6/7: √âp phi√™n b·∫£n NumPy 1.26.4...\nüîß B∆∞·ªõc 7/7: Recompile packages ph·ª• thu·ªôc numpy...\n\n============================================================\nüîç KI·ªÇM TRA PHI√äN B·∫¢N (FINAL CHECK):\n============================================================\n    ‚úÖ NumPy (Target: 1.26.4): 2.2.6\n    ‚úÖ Diffusers (Target: 0.27.2): 0.27.2\n    ‚úÖ HuggingFace Hub (Target: <0.25): 0.24.7\n    ‚úÖ PEFT (Target: 0.8.2): 0.8.2\n============================================================\n‚úÖ QU√Å TR√åNH C√ÄI ƒê·∫∂T HO√ÄN T·∫§T!\n\n‚ö†Ô∏è  B∆Ø·ªöC QUY·∫æT ƒê·ªäNH: RESTART SESSION NGAY.\n============================================================\n    üëâ Ch·ªçn Menu: Run -> Restart Session\n    üëâ Sau khi Restart: Ch·∫°y ti·∫øp Cell 3 (ƒë√£ s·ª≠a ·ªü tr√™n), r·ªìi Cell 4, 5.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# @title 3Ô∏è‚É£ PATCH: V√° l·ªói TO√ÄN DI·ªÜN (B·∫£n \"Direct Import\" - B·ªè qua torch.hub)\nimport os\nimport subprocess\nimport sys\nimport shutil\n\nWORK_DIR = \"/kaggle/temp/Depth-Aware-Editing\"\nif os.path.exists(WORK_DIR):\n    os.chdir(WORK_DIR)\nelse:\n    raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y {WORK_DIR}. Ch·∫°y l·∫°i Cell 1!\")\n\nprint(\"üöë ƒêang ch·∫°y b·∫£n v√° l·ªói (Chi·∫øn thu·∫≠t: Direct Import - Lo·∫°i b·ªè torch.hub)...\")\n\ndef run_cmd(cmd):\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# ============================================================\n# 1. C√ÄI TH∆Ø VI·ªÜN & SETUP DINOv2 (QUAN TR·ªåNG)\n# ============================================================\nprint(\"    üì• [1/10] Setup m√¥i tr∆∞·ªùng & DINOv2...\")\nrun_cmd(\"pip install open-clip-torch\")\n\n# Clone v√† copy DINOv2 ra root ƒë·ªÉ Python nh√¨n th·∫•y tr·ª±c ti·∫øp\ndinov2_repo = os.path.join(os.getcwd(), \"dinov2_repo\")\nif not os.path.exists(dinov2_repo):\n    subprocess.run(f\"git clone -q https://github.com/facebookresearch/dinov2.git {dinov2_repo}\", shell=True)\n\n# Copy folder 'dinov2' (package) ra th∆∞ m·ª•c g·ªëc\nsource_dinov2 = os.path.join(dinov2_repo, \"dinov2\")\ntarget_dinov2 = os.path.join(os.getcwd(), \"dinov2\")\n\nif os.path.exists(source_dinov2):\n    if os.path.exists(target_dinov2):\n        shutil.rmtree(target_dinov2)\n    shutil.copytree(source_dinov2, target_dinov2)\n    print(\"      ‚úÖ ƒê√£ ƒë·∫∑t DINOv2 v√†o v·ªã tr√≠ chu·∫©n.\")\n\n# ============================================================\n# 2. V√Å FILE modules.py (LO·∫†I B·ªé torch.hub.load)\n# ============================================================\nprint(\"    üîß [2/10] Ph·∫´u thu·∫≠t file modules.py (Bypass torch.hub)...\")\nmodules_file = \"ldm/modules/encoders/modules.py\"\n\nif os.path.exists(modules_file):\n    with open(modules_file, \"r\") as f:\n        code = f.read()\n\n    # ƒêo·∫°n code m·ªõi: Import tr·ª±c ti·∫øp thay v√¨ d√πng torch.hub\n    header_patch = \"\"\"\nimport sys, os\n# ƒê·∫£m b·∫£o Python t√¨m th·∫•y dinov2 ·ªü th∆∞ m·ª•c hi·ªán t·∫°i\nsys.path.append(os.getcwd()) \ntry:\n    from dinov2.hub.backbones import dinov2_vitg14\nexcept ImportError:\n    print(\"‚ö†Ô∏è Warning: Import dinov2 failed initially, retrying...\")\n    from dinov2_repo.dinov2.hub.backbones import dinov2_vitg14\n\"\"\"\n    if \"from dinov2.hub.backbones\" not in code:\n        code = header_patch + \"\\n\" + code\n\n    # Thay th·∫ø chu·ªói string g·ªçi h√†m\n    target_str_1 = \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\"\n    target_str_2 = \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14', source='local', pretrained=False)\"\n    target_str_3 = f\"torch.hub.load('{dinov2_repo}', 'dinov2_vitg14', source='local', pretrained=False)\"\n    \n    replacement = \"dinov2_vitg14(pretrained=False)\"\n    \n    code = code.replace(target_str_1, replacement)\n    code = code.replace(target_str_2, replacement)\n    code = code.replace(target_str_3, replacement)\n    \n    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p code ƒë√£ b·ªã s·ª≠a b·ªüi cell tr∆∞·ªõc (lambda) ho·∫∑c bi·∫øn th·ªÉ kh√°c\n    if \"source='local'\" in code and \"dinov2_vitg14\" in code:\n        lines = code.split('\\n')\n        new_lines = []\n        for line in lines:\n            if \"torch.hub.load\" in line and \"dinov2\" in line:\n                indent = line[:len(line) - len(line.lstrip())]\n                # Thay th·∫ø tr·ª±c ti·∫øp d√≤ng g·ªçi l·ªánh c≈© b·∫±ng l·ªánh m·ªõi\n                new_line = line.replace(\"torch.hub.load('/kaggle/temp/Depth-Aware-Editing/dinov2_repo', 'dinov2_vitg14', source='local', pretrained=False)\", replacement)\n                \n                # Fix cho c√°c bi·∫øn th·ªÉ kh√°c n·∫øu replace tr√™n kh√¥ng b·∫Øt ƒë∆∞·ª£c\n                if \"torch.hub.load\" in new_line: \n                     new_line = indent + f\"dinov2 = {replacement}\" # <--- ƒê√É S·ª¨A L·ªñI ·ªû ƒê√ÇY (th√™m d·∫•u ngo·∫∑c k√©p)\n                \n                new_lines.append(new_line)\n            else:\n                new_lines.append(line)\n        code = '\\n'.join(new_lines)\n\n    # X√≥a s·∫°ch c√°c d√≤ng import hubconf th·ª´a th√£i\n    code = code.replace(\"import hubconf\", \"# import hubconf\")\n    \n    with open(modules_file, \"w\") as f:\n        f.write(code)\n    print(\"      ‚úÖ ƒê√£ thay th·∫ø torch.hub.load b·∫±ng Direct Import.\")\n\n# ============================================================\n# 3. FIX C√ÅC L·ªñI KH√ÅC (NH∆Ø C≈®)\n# ============================================================\nprint(\"    üîÑ [3/10] Fix datasets, PositionNet, CaptionProjection...\")\nif os.path.exists(\"datasets\") and os.path.isdir(\"datasets\"):\n    shutil.move(\"datasets\", \"repo_datasets\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/from datasets/from repo_datasets/g'\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/import datasets/import repo_datasets/g'\")\n\n# Fix UNet\ntarget_unet = \"src/featglac/model/unet_2d_condition.py\"\nif os.path.exists(target_unet):\n    with open(target_unet, \"r\") as f: code = f.read()\n    if \"PositionNet,\" in code: code = code.replace(\"PositionNet,\", \"\")\n    dummy_pos = \"import torch.nn as nn\\nclass PositionNet(nn.Module):\\n    def __init__(self, *args, **kwargs): super().__init__()\\n    def forward(self, x, *args, **kwargs): return x\"\n    if \"class PositionNet\" not in code:\n        code = code.replace(\"from diffusers.models.embeddings import (\", dummy_pos + \"\\nfrom diffusers.models.embeddings import (\")\n    with open(target_unet, \"w\") as f: f.write(code)\n\n# Fix Transformer\ntarget_trans = \"src/featglac/model/transformer_2d.py\"\nif os.path.exists(target_trans):\n    with open(target_trans, \"r\") as f: code = f.read()\n    caption_proj = \"import torch.nn as nn\\nclass CaptionProjection(nn.Module):\\n    def __init__(self, i, h): super().__init__(); self.l1=nn.Linear(i,h); self.a=nn.GELU(); self.l2=nn.Linear(h,h)\\n    def forward(self, x): return self.l2(self.a(self.l1(x)))\"\n    if \"class CaptionProjection\" not in code:\n        code = code.replace(\"from diffusers.models.embeddings import CaptionProjection, PatchEmbed\", caption_proj + \"\\nfrom diffusers.models.embeddings import PatchEmbed\")\n        code = code.replace(\"CaptionProjection,\", \"\")\n        if \"class CaptionProjection\" not in code: code = caption_proj + \"\\n\" + code\n    with open(target_trans, \"w\") as f: f.write(code)\n\n# Fix Config & PL\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's|stabilityai/stable-diffusion-2-1-base|runwayml/stable-diffusion-v1-5|g'\")\nrun_cmd(\"find . -name '*.yaml' -print0 | xargs -0 sed -i 's|stabilityai/stable-diffusion-2-1-base|runwayml/stable-diffusion-v1-5|g'\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/pytorch_lightning.utilities.distributed/pytorch_lightning.utilities.rank_zero/g'\")\n\n# ============================================================\n# 4. VI·∫æT L·∫†I FILE grounded_sam_demo1.py\n# ============================================================\nprint(\"    üìù [4/10] Vi·∫øt file grounded_sam_demo1.py...\")\ndemo_code = r'''\nimport sys, os\nsys.path.append(os.path.join(os.getcwd(), 'src', 'grounded_sam')); sys.path.append(os.path.join(os.getcwd(), 'src', 'grounded_sam', 'GroundingDINO'))\nimport argparse, json, numpy as np, torch, cv2\nfrom PIL import Image; import matplotlib.pyplot as plt\nimport GroundingDINO.groundingdino.datasets.transforms as T\nfrom GroundingDINO.groundingdino.models import build_model; from GroundingDINO.groundingdino.util.slconfig import SLConfig\nfrom GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\ntry: from segment_anything import sam_model_registry, sam_hq_model_registry, SamPredictor\nexcept: from segment_anything import sam_model_registry, SamPredictor; sam_hq_model_registry = None\ndef load_image(p): i = Image.open(p).convert(\"RGB\"); t = T.Compose([T.RandomResize([800], max_size=1333), T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]); return i, t(i, None)[0]\ndef load_model(c, ck, b, d): a=SLConfig.fromfile(c); a.device=d; a.bert_base_uncased_path=b; m=build_model(a); m.load_state_dict(clean_state_dict(torch.load(ck, map_location=\"cpu\")[\"model\"]), strict=False); m.eval(); return m\ndef get_grounding_output(m, i, c, b, t, w=True, d=\"cpu\"):\n    c=c.lower().strip(); c=c+\".\" if not c.endswith(\".\") else c; m=m.to(d); i=i.to(d)\n    with torch.no_grad(): o=m(i[None], captions=[c])\n    l=o[\"pred_logits\"].cpu().sigmoid()[0]; bx=o[\"pred_boxes\"].cpu()[0]; f=l.max(dim=1)[0]>b; lf=l[f]; bf=bx[f]; tok=m.tokenizer(c); p=[]\n    for lg in lf: ph=get_phrases_from_posmap(lg>t, tok, m.tokenizer); p.append(ph+f\"({str(lg.max().item())[:4]})\" if w else ph)\n    return bf, p\ndef main(a):\n    os.makedirs(a.output_dir, exist_ok=True); ip, i=load_image(a.input_image); m=load_model(a.config, a.grounded_checkpoint, a.bert_base_uncased_path, a.device)\n    bf, pp=get_grounding_output(m, i, a.text_prompt, a.box_threshold, a.text_threshold, device=a.device)\n    s=(sam_hq_model_registry if a.use_sam_hq else sam_model_registry)[a.sam_version](checkpoint=a.sam_hq_checkpoint if a.use_sam_hq else a.sam_checkpoint).to(a.device)\n    pr=SamPredictor(s); ic=cv2.cvtColor(cv2.imread(a.input_image), cv2.COLOR_BGR2RGB); pr.set_image(ic)\n    H,W=ip.size[1],ip.size[0]\n    for k in range(bf.size(0)): bf[k]=bf[k]*torch.Tensor([W,H,W,H]); bf[k][:2]-=bf[k][2:]/2; bf[k][2:]+=bf[k][:2]\n    tb=pr.transform.apply_boxes_torch(bf, ic.shape[:2]).to(a.device); ma,_,_=pr.predict_torch(None, None, boxes=tb, multimask_output=False)\n    plt.figure(figsize=(10,10)); plt.imshow(ic)\n    for mk in ma: c=np.concatenate([np.random.random(3), np.array([0.6])]); h,w=mk.shape[-2:]; plt.gca().imshow(mk.cpu().numpy().reshape(h,w,1)*c.reshape(1,1,-1))\n    for bx,lb in zip(bf,pp): x,y,w,h=bx[0],bx[1],bx[2]-bx[0],bx[3]-bx[1]; plt.gca().add_patch(plt.Rectangle((x,y),w,h,edgecolor='green',facecolor='none',lw=2)); plt.gca().text(x,y,lb)\n    plt.axis('off'); plt.savefig(os.path.join(a.output_dir, \"grounded_sam_output.jpg\"), bbox_inches=\"tight\", pad_inches=0); plt.close()\nif __name__==\"__main__\": pass \n'''\nwith open(\"src/grounded_sam/grounded_sam_demo1.py\", \"w\") as f: f.write(demo_code)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ƒê√É V√Å XONG (Direct Import Strategy)!\")\nprint(\"üëâ ƒê√¢y l√† bi·ªán ph√°p m·∫°nh nh·∫•t. H√£y ch·∫°y l·∫°i Cell 4.\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:19:03.098087Z","iopub.execute_input":"2025-12-25T01:19:03.098323Z","iopub.status.idle":"2025-12-25T01:19:08.031425Z","shell.execute_reply.started":"2025-12-25T01:19:03.098300Z","shell.execute_reply":"2025-12-25T01:19:08.030801Z"}},"outputs":[{"name":"stdout","text":"üöë ƒêang ch·∫°y b·∫£n v√° l·ªói (Chi·∫øn thu·∫≠t: Direct Import - Lo·∫°i b·ªè torch.hub)...\n    üì• [1/10] Setup m√¥i tr∆∞·ªùng & DINOv2...\n      ‚úÖ ƒê√£ ƒë·∫∑t DINOv2 v√†o v·ªã tr√≠ chu·∫©n.\n    üîß [2/10] Ph·∫´u thu·∫≠t file modules.py (Bypass torch.hub)...\n      ‚úÖ ƒê√£ thay th·∫ø torch.hub.load b·∫±ng Direct Import.\n    üîÑ [3/10] Fix datasets, PositionNet, CaptionProjection...\n    üìù [4/10] Vi·∫øt file grounded_sam_demo1.py...\n\n============================================================\n‚úÖ ƒê√É V√Å XONG (Direct Import Strategy)!\nüëâ ƒê√¢y l√† bi·ªán ph√°p m·∫°nh nh·∫•t. H√£y ch·∫°y l·∫°i Cell 4.\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# @title 4Ô∏è‚É£ WEIGHTS: Chu·∫©n b·ªã Model\nimport os\n\nos.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\nweights_dir = \"weights\"\nos.makedirs(weights_dir, exist_ok=True)\n\nprint(\"üèãÔ∏è‚Äç‚ôÇÔ∏è ƒêang chu·∫©n b·ªã Model...\")\n\nmodels = {\n    \"depth_anything_metric_depth_indoor.pt\": (\"depth_anything_metric_depth_indoor\", \"https://huggingface.co/LiheYoung/Depth-Anything/resolve/main/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt\"),\n    \"depth_anything_vitl14.pth\": (\"depth_anything_vitl14\", \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vitl14.pth\"),\n    \"dinov2_vitg14_pretrain.pth\": (\"dinov2_vitg14\", \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth\"),\n    \"epoch=1-step=8687.ckpt\": (\"8687\", \"https://huggingface.co/spaces/xichenhku/AnyDoor/resolve/main/epoch%3D1-step%3D8687.ckpt\"),\n    \"sd-v1-5-inpainting.ckpt\": (\"inpainting\", \"https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt\")\n}\n\nfor target, (key, url) in models.items():\n    dest = os.path.join(weights_dir, target)\n    found = False\n    for root, _, files in os.walk(\"/kaggle/input\"):\n        for f in files:\n            if key in f:\n                if os.path.exists(dest) or os.path.islink(dest): os.remove(dest)\n                os.symlink(os.path.join(root, f), dest)\n                print(f\"üîó Linked: {target}\")\n                found = True\n                break\n        if found: break\n    \n    if not found:\n        if not os.path.exists(dest) or os.path.getsize(dest) < 1024*1024:\n            print(f\"‚¨áÔ∏è Downloading: {target}...\")\n            os.system(f\"wget -q -O '{dest}' '{url}'\")\n        else:\n            print(f\"‚úÖ ƒê√£ c√≥ s·∫µn: {target}\")\n\nprint(\"‚úÖ B∆Ø·ªöC 4 HO√ÄN T·∫§T.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:19:08.034907Z","iopub.execute_input":"2025-12-25T01:19:08.035140Z","iopub.status.idle":"2025-12-25T01:27:59.941381Z","shell.execute_reply.started":"2025-12-25T01:19:08.035118Z","shell.execute_reply":"2025-12-25T01:27:59.940686Z"}},"outputs":[{"name":"stdout","text":"üèãÔ∏è‚Äç‚ôÇÔ∏è ƒêang chu·∫©n b·ªã Model...\nüîó Linked: depth_anything_metric_depth_indoor.pt\nüîó Linked: depth_anything_vitl14.pth\nüîó Linked: dinov2_vitg14_pretrain.pth\n‚¨áÔ∏è Downloading: epoch=1-step=8687.ckpt...\n‚¨áÔ∏è Downloading: sd-v1-5-inpainting.ckpt...\n‚úÖ B∆Ø·ªöC 4 HO√ÄN T·∫§T.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# @title 5Ô∏è‚É£ SETUP MODULE 1: Segmentation (FIX: Size Mismatch & NumPy Types)\nimport os\nimport sys\n\n# 1. ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n\nif os.path.exists(\"/kaggle/temp/Depth-Aware-Editing\"):\n    os.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n\nprint(\"üöÄ ƒêang thi·∫øt l·∫≠p Module 1 & V√° l·ªói k√≠ch th∆∞·ªõc ·∫£nh...\")\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 1: T·∫†O HUBCONF\n# ------------------------------------------------------------------------------\nhubconf_code = \"\"\"\nimport torch\nimport os\ndef dinov2_vitg14(pretrained=False, **kwargs):\n    local_repo = os.path.abspath(\"dinov2_repo\")\n    if os.path.exists(local_repo):\n        try: return torch.hub.load(local_repo, 'dinov2_vitg14', source='local', pretrained=False, **kwargs)\n        except: pass\n    return torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14', **kwargs)\n\"\"\"\nwith open(\"hubconf.py\", \"w\") as f: f.write(hubconf_code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 2: V√Å FILE MODULES\n# ------------------------------------------------------------------------------\ntarget_file = \"ldm/modules/encoders/modules.py\"\nif os.path.exists(target_file):\n    with open(target_file, \"r\") as f: code = f.read()\n    if \"import hubconf\" not in code:\n        lines = code.split('\\n')\n        for i, line in enumerate(lines):\n            if line.strip().startswith((\"import\", \"from\")):\n                lines.insert(i, \"import os, sys; sys.path.append(os.getcwd()); import hubconf\")\n                break\n        code = '\\n'.join(lines)\n    if \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\" in code:\n        code = code.replace(\"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\", \"hubconf.dinov2_vitg14()\")\n    with open(target_file, \"w\") as f: f.write(code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 3: V√Å L·ªñI NUMPY INT64 (Fix l·ªói n·ªôi t·∫°i c·ªßa th∆∞ vi·ªán Depth)\n# ------------------------------------------------------------------------------\nda_file = \"src/Depth-Anything/metric_depth/zoedepth/models/base_models/depth_anything.py\"\nif os.path.exists(da_file):\n    with open(da_file, \"r\") as f: da_code = f.read()\n    target_str = \"return nn.functional.interpolate(x, (height, width), mode='bilinear', align_corners=True)\"\n    replace_str = \"return nn.functional.interpolate(x, (int(height), int(width)), mode='bilinear', align_corners=True)\"\n    if target_str in da_code:\n        da_code = da_code.replace(target_str, replace_str)\n        with open(da_file, \"w\") as f: f.write(da_code)\n    elif \"(height, width)\" in da_code: # Fallback\n        da_code = da_code.replace(\"(height, width)\", \"(int(height), int(width))\")\n        with open(da_file, \"w\") as f: f.write(da_code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 4: T·∫†O FILE RUN GRADIO (ƒê√£ th√™m l·ªánh resize ƒë·ªìng b·ªô)\n# ------------------------------------------------------------------------------\ngradio_code = r'''\nimport os\nimport sys\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport gradio as gr\n\n# Setup path\nsys.path.append(os.getcwd())\nsys.path.append(os.path.join(os.getcwd(), \"src\", \"grounded_sam\"))\n\n# Import functions\nfrom utils.mpi.preprocess import get_depth_and_sam_mask\nfrom utils.mpi.mpi import get_mpi_rgb_and_alpha\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Module 1 Environment: {device}\")\n\ndef run_segmentation(input_image, depth_threshold):\n    if input_image is None:\n        raise gr.Error(\"Vui l√≤ng upload ·∫£nh!\")\n    \n    # 1. Resize ·∫£nh ƒë·∫ßu v√†o (Input)\n    w, h = input_image.size\n    max_s = 1024\n    if max(w, h) > max_s:\n        scale = max_s / max(w, h)\n        input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n    \n    print(\"running get_depth_and_sam_mask...\")\n    # L·∫•y depth map t·ª´ model (Model c√≥ th·ªÉ tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc h∆°i kh√°c)\n    depth_img, sam_mask_img = get_depth_and_sam_mask(input_image, False)\n    \n    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY (QUAN TR·ªåNG) ---\n    # √âp bu·ªôc depth_img ph·∫£i c√≥ k√≠ch th∆∞·ªõc Y H·ªÜT input_image\n    if depth_img.size != input_image.size:\n        print(f\"‚ö†Ô∏è Resizing depth from {depth_img.size} to match input {input_image.size}\")\n        depth_img = depth_img.resize(input_image.size, Image.NEAREST)\n    # ------------------------------------\n\n    rgb_np = np.array(input_image)\n    depth_np = np.array(depth_img)\n    \n    # Ki·ªÉm tra l·∫ßn cu·ªëi\n    if rgb_np.shape[:2] != depth_np.shape[:2]:\n        raise ValueError(f\"Shape mismatch: RGB {rgb_np.shape} vs Depth {depth_np.shape}\")\n\n    # T√°ch l·ªõp\n    thresholds = [(0, depth_threshold), (depth_threshold, 255)]\n    print(f\"running get_mpi_rgb_and_alpha with threshold {depth_threshold}...\")\n    \n    try:\n        mpi_rgb, mpi_alpha = get_mpi_rgb_and_alpha(rgb_np, depth_np, thresholds)\n    except cv2.error as e:\n        print(\"OpenCV Error handled:\", e)\n        # Fallback th·ªß c√¥ng n·∫øu OpenCV v·∫´n l·ªói (hi·∫øm khi x·∫£y ra n·∫øu ƒë√£ resize ·ªü tr√™n)\n        mask = (depth_np > depth_threshold).astype(np.uint8) * 255\n        mpi_rgb = [cv2.bitwise_and(rgb_np, rgb_np, mask=255-mask), cv2.bitwise_and(rgb_np, rgb_np, mask=mask)]\n        mpi_alpha = [(255-mask)/255.0, mask/255.0]\n\n    foreground = mpi_rgb[0]\n    background = mpi_rgb[1]\n    fg_mask = mpi_alpha[0] * 255\n    \n    return [\n        depth_img,\n        sam_mask_img,\n        Image.fromarray(foreground.astype(np.uint8)),\n        Image.fromarray(background.astype(np.uint8)),\n        Image.fromarray(fg_mask.astype(np.uint8))\n    ]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üîç Module 1: Segmentation & MPI Splitting\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            input_img = gr.Image(label=\"Input Image\", type=\"pil\", height=400)\n            d_thresh = gr.Slider(0, 255, value=50, step=1, label=\"Depth Threshold\")\n            btn = gr.Button(\"‚úÇÔ∏è T√°ch L·ªõp\", variant=\"primary\")\n        \n        with gr.Column(scale=2):\n            gallery = gr.Gallery(label=\"K·∫øt qu·∫£\", columns=3, height=600, object_fit=\"contain\")\n\n    btn.click(fn=run_segmentation, inputs=[input_img, d_thresh], outputs=[gallery])\n\nif __name__ == \"__main__\":\n    demo.queue().launch(share=True, debug=True, allowed_paths=[\"/kaggle/temp\"])\n'''\n\nwith open(\"gradio_segmentation.py\", \"w\") as f: f.write(gradio_code)\nprint(\"‚úÖ ƒê√£ s·ª≠a l·ªói k√≠ch th∆∞·ªõc ·∫£nh (Size Mismatch Fixed).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:27:59.943066Z","iopub.execute_input":"2025-12-25T01:27:59.943504Z","iopub.status.idle":"2025-12-25T01:28:02.773447Z","shell.execute_reply.started":"2025-12-25T01:27:59.943475Z","shell.execute_reply":"2025-12-25T01:28:02.772549Z"}},"outputs":[{"name":"stdout","text":"üöÄ ƒêang thi·∫øt l·∫≠p Module 1 & V√° l·ªói k√≠ch th∆∞·ªõc ·∫£nh...\n‚úÖ ƒê√£ s·ª≠a l·ªói k√≠ch th∆∞·ªõc ·∫£nh (Size Mismatch Fixed).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 4: T·∫†O FILE RUN GRADIO (C√ì T√çNH NƒÇNG L∆ØU FILE + RAW DEPTH CHO MODULE 2,4)\n# ------------------------------------------------------------------------------\ngradio_code = r'''\nimport os\nimport sys\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport gradio as gr\nimport matplotlib\nmatplotlib.use('Agg') # Backend kh√¥ng c·∫ßn m√†n h√¨nh\nimport matplotlib.pyplot as plt\nimport io\nimport traceback\n\n# Setup path\nsys.path.append(os.getcwd())\nsys.path.append(os.path.join(os.getcwd(), \"src\", \"grounded_sam\"))\n\nfrom utils.mpi.preprocess import get_depth_and_sam_mask\nfrom utils.mpi.mpi import get_mpi_rgb_and_alpha\n\n# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\nOUTPUT_DIR = \"outputs/module1\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"üìÇ K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(OUTPUT_DIR)}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- LOAD ZOEDEPTH (KITTI) ---\nprint(\"‚è≥ ƒêang load model ZoeDepth (ZoeD_K)...\")\nzoe_model = None\ntry:\n    zoe_model = torch.hub.load(\"isl-org/ZoeDepth\", \"ZoeD_K\", pretrained=True).to(device).eval()\n    print(\"‚úÖ Load ZoeDepth KITTI th√†nh c√¥ng!\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è L·ªói load model (s·∫Ω d√πng fallback): {e}\")\n\ndef ensure_pil(img):\n    if isinstance(img, tuple) or isinstance(img, list): img = img[0]\n    if isinstance(img, np.ndarray):\n        if img.dtype != np.uint8:\n            img = (img - img.min()) / (img.max() - img.min() + 1e-5) * 255\n            img = img.astype(np.uint8)\n        img = Image.fromarray(img)\n    if torch.is_tensor(img):\n        img = img.cpu().detach().numpy()\n        img = (img * 255).astype(np.uint8)\n        img = Image.fromarray(img)\n    return img\n\ndef colorize_depth_map(depth_np, is_metric=True):\n    plt.figure(figsize=(10, 8))\n    if isinstance(depth_np, torch.Tensor): depth_np = depth_np.squeeze().cpu().numpy()\n    \n    min_val, max_val = np.min(depth_np), np.max(depth_np)\n    plt.imshow(depth_np, cmap='jet')\n    \n    cbar = plt.colorbar(fraction=0.046, pad=0.04)\n    if is_metric:\n        cbar.set_label('Kho·∫£ng c√°ch th·ª±c t·∫ø (M√©t)', rotation=270, labelpad=20)\n        plt.title(f\"ZoeDepth KITTI (Min: {min_val:.1f}m - Max: {max_val:.1f}m)\")\n    else:\n        cbar.set_label('Gi√° tr·ªã ƒëi·ªÉm ·∫£nh (Relative)', rotation=270, labelpad=20)\n        plt.title(f\"Relative Depth (Colorized)\")\n\n    plt.axis('off')\n    plt.tight_layout()\n    \n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n    buf.seek(0)\n    img_color = Image.open(buf).convert(\"RGB\")\n    plt.close()\n    return img_color\n\n# --- C·∫¨P NH·∫¨T: TH√äM THAM S·ªê background_img ---\ndef save_outputs(input_img, mask_img, depth_vis_img, foreground_img, background_img, raw_depth_np):\n    \"\"\"L∆∞u c√°c file c·∫ßn thi·∫øt cho Module 2 v√† Module 4\"\"\"\n    try:\n        # 1. L∆∞u ·∫£nh (Visual)\n        input_img.save(os.path.join(OUTPUT_DIR, \"input.png\"))\n        mask_img.save(os.path.join(OUTPUT_DIR, \"mask.png\"))\n        depth_vis_img.save(os.path.join(OUTPUT_DIR, \"depth_vis.png\"))\n        foreground_img.save(os.path.join(OUTPUT_DIR, \"foreground.png\"))\n        \n        # --- L∆ØU BACKGROUND ---\n        background_img.save(os.path.join(OUTPUT_DIR, \"background.png\"))\n        \n        # 2. L∆ØU QUAN TR·ªåNG: RAW DEPTH (NUMPY)\n        # File n√†y c·∫ßn cho Module 4 t√≠nh to√°n 3D\n        np.save(os.path.join(OUTPUT_DIR, \"depth.npy\"), raw_depth_np)\n        \n        print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ (bao g·ªìm background.png v√† depth.npy) v√†o {OUTPUT_DIR}\")\n    except Exception as e:\n        print(f\"‚ùå L·ªói khi l∆∞u file: {e}\")\n        import traceback\n        traceback.print_exc()\n\ndef run_segmentation(input_image, depth_threshold):\n    if input_image is None: raise gr.Error(\"Ch∆∞a upload ·∫£nh!\")\n    \n    # Resize v·ªÅ chu·∫©n Model (tr√°nh qu√° to ho·∫∑c l·∫ª)\n    w, h = input_image.size\n    max_s = 1024\n    if max(w, h) > max_s:\n        scale = max_s / max(w, h)\n        input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n    \n    w, h = input_image.size # C·∫≠p nh·∫≠t l·∫°i size\n    \n    # Bi·∫øn l∆∞u tr·ªØ depth th√¥ (ƒë·ªÉ l∆∞u file .npy)\n    final_raw_depth = None \n    depth_metric_img = None\n    \n    # 1. Metric Depth (ZoeDepth)\n    if zoe_model is not None:\n        try:\n            with torch.no_grad():\n                depth_tensor = zoe_model.infer_pil(input_image)\n            if isinstance(depth_tensor, torch.Tensor): raw_metric = depth_tensor.cpu().numpy()\n            else: raw_metric = depth_tensor\n            \n            # Resize depth v·ªÅ ƒë√∫ng size ·∫£nh input (n·∫øu Zoe tr·∫£ v·ªÅ size kh√°c)\n            if raw_metric.shape[-2:] != (h, w):\n                 raw_metric = cv2.resize(raw_metric, (w, h), interpolation=cv2.INTER_LINEAR)\n            \n            final_raw_depth = raw_metric # L∆∞u l·∫°i depth th√¥ n√†y\n            depth_metric_img = colorize_depth_map(raw_metric, is_metric=True)\n        except Exception as e:\n            print(f\"Zoe Error: {e}\")\n            depth_metric_img = None\n\n    # 2. Segmentation (SAM / Pretrained)\n    print(\"üöÄ Running Segmentation...\")\n    raw_results = get_depth_and_sam_mask(input_image, False)\n    depth_raw_rel = raw_results[0]\n    sam_mask_raw = raw_results[1]\n    \n    depth_img_rel = ensure_pil(depth_raw_rel)\n    sam_mask_img = ensure_pil(sam_mask_raw)\n    \n    # Resize mask v·ªÅ ƒë√∫ng size ·∫£nh input\n    if sam_mask_img.size != input_image.size:\n        sam_mask_img = sam_mask_img.resize(input_image.size, Image.NEAREST)\n\n    # Fallback Depth: N·∫øu Zoe l·ªói, d√πng depth t∆∞∆°ng ƒë·ªëi\n    if final_raw_depth is None:\n        final_raw_depth = np.array(depth_img_rel).astype(np.float32) / 255.0 # Chu·∫©n h√≥a v·ªÅ 0-1\n        if depth_metric_img is None:\n            depth_metric_img = colorize_depth_map(np.array(depth_img_rel), is_metric=False)\n\n    # 3. MPI / Cutout (T√°ch n·ªÅn d·ª±a tr√™n ng∆∞·ª°ng)\n    rgb_np = np.array(input_image)\n    depth_np_rel = np.array(depth_img_rel)\n    \n    # Convert mask SAM sang binary (0-255)\n    sam_mask_np = np.array(sam_mask_img)\n    if len(sam_mask_np.shape) == 3: sam_mask_np = sam_mask_np[:,:,0]\n    \n    # T·∫°o ·∫£nh Foreground (Ch·ªâ gi·ªØ l·∫°i ph·∫ßn trong mask)\n    fg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n    fg_rgba[:, :, :3] = rgb_np\n    fg_rgba[:, :, 3] = sam_mask_np # Alpha channel\n    \n    fg_img = Image.fromarray(fg_rgba)\n    \n    # T·∫°o ·∫£nh Background (Ph·∫ßn c√≤n l·∫°i)\n    bg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n    bg_rgba[:, :, :3] = rgb_np\n    bg_rgba[:, :, 3] = 255 - sam_mask_np\n    bg_img = Image.fromarray(bg_rgba)\n    \n    alpha_img = Image.fromarray(sam_mask_np)\n\n    # --- L∆ØU OUTPUT QUAN TR·ªåNG (ƒê√É TH√äM BG_IMG) ---\n    save_outputs(input_image, sam_mask_img, depth_metric_img, fg_img, bg_img, final_raw_depth)\n    # -------------------------------------------\n\n    return [depth_metric_img, sam_mask_img, fg_img, bg_img, alpha_img]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üîç Module 1: Outdoor Segmentation & ZoeDepth (Fixed Auto-Save)\")\n    gr.Markdown(\"K·∫øt qu·∫£ (bao g·ªìm depth map th√¥) s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o `outputs/module1`.\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            input_img = gr.Image(label=\"Input Image\", type=\"pil\", height=400)\n            d_thresh = gr.Slider(0, 255, value=100, step=1, label=\"Ng∆∞·ª°ng t√°ch l·ªõp (Kh√¥ng d√πng n·∫øu c√≥ SAM)\")\n            btn = gr.Button(\"üöÄ X·ª≠ l√Ω & L∆∞u\", variant=\"primary\")\n        \n        with gr.Column(scale=2):\n            gallery = gr.Gallery(label=\"K·∫øt qu·∫£ (Depth, Mask, FG, BG, Alpha)\", columns=3, height=600, object_fit=\"contain\")\n\n    btn.click(fn=run_segmentation, inputs=[input_img, d_thresh], outputs=[gallery])\n\nif __name__ == \"__main__\":\n    demo.queue().launch(share=True, debug=True, allowed_paths=[\"/kaggle/temp\"])\n'''\n\nwith open(\"gradio_segmentation.py\", \"w\") as f: f.write(gradio_code)\nprint(\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t Gradio: ƒê√£ s·ª≠a l·ªói l∆∞u thi·∫øu background.png.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:28:02.774764Z","iopub.execute_input":"2025-12-25T01:28:02.775143Z","iopub.status.idle":"2025-12-25T01:28:03.997805Z","shell.execute_reply.started":"2025-12-25T01:28:02.775105Z","shell.execute_reply":"2025-12-25T01:28:03.996913Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ƒê√£ c·∫≠p nh·∫≠t Gradio: ƒê√£ s·ª≠a l·ªói l∆∞u thi·∫øu background.png.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# @title 6Ô∏è‚É£ RUN MODULE 1 (METRICS & TIMER): L·∫•y s·ªë li·ªáu th·ª±c t·∫ø (Fg, Bg, Depth)\nimport os\nimport sys\nimport gc\nimport glob\nimport shutil\nimport ctypes\nimport logging\nimport numpy as np\nimport torch\nimport subprocess\nimport time\nfrom PIL import Image\n\n# ==============================================================================\n# üõ†Ô∏è B∆Ø·ªöC 0: V√Å L·ªñI SOURCE CODE (AUTO-PATCH)\n# ==============================================================================\nprint(\"üîß ƒêang v√° l·ªói 'device=0' trong utils/mpi/preprocess.py ...\")\ntarget_file = \"/kaggle/working/Depth-Aware-Editing/utils/mpi/preprocess.py\"\nif os.path.exists(target_file):\n    with open(target_file, 'r') as f:\n        code_content = f.read()\n    if \"import torch\" not in code_content:\n        code_content = \"import torch\\n\" + code_content\n    if 'device=0)' in code_content:\n        code_content = code_content.replace('device=0)', 'device=0 if torch.cuda.is_available() else -1)')\n        print(\"‚úÖ ƒê√£ s·ª≠a: device=0 -> device=dynamic\")\n    with open(target_file, 'w') as f:\n        f.write(code_content)\n\n# ==============================================================================\n# üõ†Ô∏è B∆Ø·ªöC 1: T·ª∞ ƒê·ªòNG T·∫¢I WEIGHTS\n# ==============================================================================\nPROJECT_DIR = \"/kaggle/working/Depth-Aware-Editing\"\nWEIGHTS_DIR = os.path.join(PROJECT_DIR, \"weights\")\nMISSING_FILE = \"depth_anything_metric_depth_indoor.pt\"\nif not os.path.exists(os.path.join(WEIGHTS_DIR, MISSING_FILE)):\n    print(f\"‚ö†Ô∏è Thi·∫øu file weights. ƒêang t·∫£i l·∫°i...\")\n    try:\n        if not os.path.exists(WEIGHTS_DIR): os.makedirs(WEIGHTS_DIR)\n        url = \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_metric_depth_indoor.pt\"\n        os.system(f\"wget {url} -P {WEIGHTS_DIR}\")\n    except Exception as e:\n        print(f\"‚ùå L·ªói t·∫£i weights: {e}\")\n\n# ==============================================================================\n# C·∫§U H√åNH & SPARK\n# ==============================================================================\ntry:\n    from pyspark.sql import SparkSession\nexcept ImportError:\n    os.system(\"pip install pyspark\")\n    from pyspark.sql import SparkSession\n\nINPUT_FOLDER = \"/kaggle/input/kitti-3d-object-detection-dataset\" # Ho·∫∑c thay b·∫±ng folder Selection n·∫øu c·∫ßn\nOUTPUT_BASE_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module1\"\nTEST_LIMIT = 50 \nSPARK_MASTER = \"local[1]\" \n\ndef clean_memory():\n    cleanup_vars = ['model', 'zoe_model', 'pipe', 'diff_handles', 'depth_anything']\n    for var in cleanup_vars:\n        if var in globals(): del globals()[var]\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ==============================================================================\n# LOGIC WORKER (C√ì T√çNH METRICS)\n# ==============================================================================\ndef spark_worker_logic(iterator):\n    import os\n    import sys\n    import torch\n    import numpy as np\n    from PIL import Image\n    import logging\n    import time\n\n    os.chdir(\"/kaggle/working/Depth-Aware-Editing\")\n    base_dir = os.getcwd()\n    src_dir = os.path.join(base_dir, \"src\")\n    paths_to_inject = [os.path.join(src_dir, \"ZoeDepth\"), os.path.join(src_dir, \"grounded_sam\"), src_dir, base_dir]\n    for p in paths_to_inject:\n        if os.path.exists(p) and p not in sys.path: sys.path.insert(0, p)\n\n    try:\n        from transformers import logging as trans_logging\n        trans_logging.set_verbosity_error()\n        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n    except: pass\n\n    try:\n        from utils.mpi.preprocess import get_depth_and_sam_mask\n    except ImportError as e:\n        yield (\"IMPORT_ERROR\", str(e), 0.0, 0.0, 0.0, 0.0, \"\")\n        return\n\n    for row in iterator:\n        image_path = row.image_path\n        img_id = os.path.basename(image_path).split('.')[0]\n        start_time = time.time()\n        \n        try:\n            # 1. ƒê·ªçc & Resize\n            input_image = Image.open(image_path).convert(\"RGB\")\n            w, h = input_image.size\n            max_s = 1024\n            if max(w, h) > max_s:\n                scale = max_s / max(w, h)\n                input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n                w, h = input_image.size\n            \n            # 2. Inference\n            depth_img_rel, sam_mask_img = get_depth_and_sam_mask(input_image, False)\n            \n            # 3. Chu·∫©n h√≥a Depth\n            final_depth = np.array(depth_img_rel).astype(np.float32) / 255.0\n            \n            # 4. X·ª≠ l√Ω Mask\n            sam_mask_img = sam_mask_img.resize((w, h), Image.NEAREST)\n            sam_mask_np = np.array(sam_mask_img)\n            if len(sam_mask_np.shape) == 3: sam_mask_np = sam_mask_np[:,:,0]\n            \n            # üî•üî•üî• T√çNH TO√ÅN S·ªê LI·ªÜU (METRICS) üî•üî•üî•\n            total_pixels = sam_mask_np.size\n            fg_pixels = np.count_nonzero(sam_mask_np) # Pixel kh√°c 0 l√† FG\n            \n            fg_percent = (fg_pixels / total_pixels) * 100\n            bg_percent = 100 - fg_percent\n            avg_depth_val = np.mean(final_depth) # Gi√° tr·ªã ƒë·ªô s√¢u trung b√¨nh\n            \n            # 5. L∆∞u k·∫øt qu·∫£\n            save_dir = os.path.join(OUTPUT_BASE_DIR, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            \n            rgb_np = np.array(input_image)\n            fg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n            fg_rgba[:, :, :3] = rgb_np\n            fg_rgba[:, :, 3] = sam_mask_np\n            \n            bg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n            bg_rgba[:, :, :3] = rgb_np\n            bg_rgba[:, :, 3] = 255 - sam_mask_np\n\n            Image.fromarray(fg_rgba).save(os.path.join(save_dir, \"foreground.png\"))\n            Image.fromarray(bg_rgba).save(os.path.join(save_dir, \"background.png\"))\n            Image.fromarray(sam_mask_np).save(os.path.join(save_dir, \"mask.png\"))\n            input_image.save(os.path.join(save_dir, \"input.png\"))\n            np.save(os.path.join(save_dir, \"depth.npy\"), final_depth)\n            \n            elapsed_time = time.time() - start_time\n            \n            # Tr·∫£ v·ªÅ: (ID, Status, Time, FG%, BG%, AvgDepth, Note)\n            yield (img_id, \"SUCCESS\", elapsed_time, fg_percent, bg_percent, avg_depth_val, save_dir)\n            \n        except Exception as e:\n            elapsed_time = time.time() - start_time\n            yield (img_id, f\"ERROR: {str(e)}\", elapsed_time, 0.0, 0.0, 0.0, \"\")\n\n# ==============================================================================\n# SPARK DRIVER\n# ==============================================================================\ndef main_spark_job():\n    clean_memory()\n    print(f\"üöÄ Kh·ªüi t·∫°o Spark Session ({SPARK_MASTER})...\")\n    spark = SparkSession.builder \\\n        .appName(\"Module1_Metrics\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    image_files = []\n    for ext in ('*.jpg', '*.jpeg', '*.png'):\n        image_files.extend(glob.glob(os.path.join(INPUT_FOLDER, \"**\", ext), recursive=True))\n    \n    if len(image_files) > TEST_LIMIT:\n        print(f\"‚ö†Ô∏è Test {TEST_LIMIT} ·∫£nh.\")\n        image_files = image_files[:TEST_LIMIT]\n        \n    if not image_files:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh!\")\n        spark.stop()\n        return\n\n    df = spark.createDataFrame([(f,) for f in image_files], [\"image_path\"])\n    df_partitioned = df.repartition(1)\n    \n    print(f\"üîÑ ƒêang x·ª≠ l√Ω {len(image_files)} ·∫£nh...\")\n    results_rdd = df_partitioned.rdd.mapPartitions(spark_worker_logic)\n    final_results = results_rdd.collect()\n    \n    # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---\n    print(\"\\n‚úÖ K·∫æT QU·∫¢ S·ªê LI·ªÜU TH·ª∞C T·∫æ:\")\n    print(\"-\" * 105)\n    print(f\"{'IMAGE ID':<15} | {'FG (%)':<10} | {'BG (%)':<10} | {'AVG DEPTH':<12} | {'TIME(s)':<10} | {'TR·∫†NG TH√ÅI'}\")\n    print(\"-\" * 105)\n    \n    success_count = 0\n    total_time = 0\n    \n    # Bi·∫øn ƒë·ªÉ t√≠nh trung b√¨nh chung\n    total_fg = 0\n    total_bg = 0\n    total_depth = 0\n    \n    for img_id, status, duration, fg, bg, depth, note in final_results:\n        time_str = f\"{duration:.2f}\" if isinstance(duration, float) else str(duration)\n        if isinstance(duration, float): total_time += duration\n        \n        if status == \"SUCCESS\": \n            success_count += 1\n            total_fg += fg\n            total_bg += bg\n            total_depth += depth\n            print(f\"{img_id[:15]:<15} | {fg:<10.2f} | {bg:<10.2f} | {depth:<12.4f} | {time_str:<10} | OK\")\n        else:\n            print(f\"{img_id[:15]:<15} | {'-':<10} | {'-':<10} | {'-':<12} | {time_str:<10} | {status[:30]}...\")\n            \n    print(\"-\" * 105)\n    print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(final_results)}\")\n    \n    if success_count > 0:\n        n = success_count\n        print(f\"\\nüìä TH·ªêNG K√ä TRUNG B√åNH (D√πng cho b√°o c√°o):\")\n        print(f\"   - Avg Foreground: {total_fg/n:.2f}%\")\n        print(f\"   - Avg Background: {total_bg/n:.2f}%\")\n        print(f\"   - Avg Depth:      {total_depth/n:.4f}\")\n        print(f\"   - Avg Time:       {total_time/n:.2f} s/·∫£nh\")\n        \n    spark.stop()\n\nif __name__ == \"__main__\":\n    if os.path.exists(OUTPUT_BASE_DIR): shutil.rmtree(OUTPUT_BASE_DIR)\n    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n    main_spark_job()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T01:41:44.183493Z","iopub.execute_input":"2025-12-25T01:41:44.184473Z","iopub.status.idle":"2025-12-25T01:49:11.733291Z","shell.execute_reply.started":"2025-12-25T01:41:44.184433Z","shell.execute_reply":"2025-12-25T01:49:11.732499Z"}},"outputs":[{"name":"stdout","text":"üîß ƒêang v√° l·ªói 'device=0' trong utils/mpi/preprocess.py ...\n‚úÖ ƒê√£ s·ª≠a: device=0 -> device=dynamic\nüöÄ Kh·ªüi t·∫°o Spark Session (local[1])...\n‚ö†Ô∏è Test 50 ·∫£nh.\nüîÑ ƒêang x·ª≠ l√Ω 50 ·∫£nh...\n","output_type":"stream"},{"name":"stderr","text":"xFormers not available                                              (0 + 1) / 1]\nxFormers not available\nParams passed to Resize transform:\n\twidth:  518\n\theight:  392\n\tresize_target:  True\n\tkeep_aspect_ratio:  False\n\tensure_multiple_of:  14\n\tresize_method:  minimal\nUsing pretrained resource local::./weights/depth_anything_metric_depth_indoor.pt\nLoaded successfully\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ K·∫æT QU·∫¢ S·ªê LI·ªÜU TH·ª∞C T·∫æ:\n---------------------------------------------------------------------------------------------------------\nIMAGE ID        | FG (%)     | BG (%)     | AVG DEPTH    | TIME(s)    | TR·∫†NG TH√ÅI\n---------------------------------------------------------------------------------------------------------\n005084          | 81.35      | 18.65      | 0.1640       | 8.10       | OK\n003065          | 42.65      | 57.35      | 0.1501       | 6.77       | OK\n003741          | 89.63      | 10.37      | 0.1797       | 7.90       | OK\n004725          | 86.27      | 13.73      | 0.1498       | 7.13       | OK\n001183          | 85.14      | 14.86      | 0.1482       | 7.31       | OK\n005660          | 93.53      | 6.47       | 0.1178       | 7.40       | OK\n007355          | 84.15      | 15.85      | 0.1427       | 7.27       | OK\n003244          | 81.36      | 18.64      | 0.1502       | 7.49       | OK\n002594          | 90.36      | 9.64       | 0.1152       | 7.44       | OK\n001703          | 83.05      | 16.95      | 0.1497       | 7.53       | OK\n003380          | 66.35      | 33.65      | 0.1223       | 7.14       | OK\n007190          | 85.76      | 14.24      | 0.1593       | 7.90       | OK\n004566          | 83.70      | 16.30      | 0.1232       | 7.57       | OK\n002584          | 96.77      | 3.23       | 0.1554       | 8.59       | OK\n004035          | 74.32      | 25.68      | 0.1636       | 7.54       | OK\n001840          | 86.31      | 13.69      | 0.1332       | 7.80       | OK\n005647          | 77.63      | 22.37      | 0.1736       | 7.48       | OK\n004924          | 80.40      | 19.60      | 0.1444       | 7.88       | OK\n005000          | 89.60      | 10.40      | 0.1496       | 7.80       | OK\n007456          | 58.08      | 41.92      | 0.1629       | 7.85       | OK\n006796          | 85.37      | 14.63      | 0.1062       | 7.92       | OK\n000419          | 97.04      | 2.96       | 0.1190       | 8.46       | OK\n002712          | 82.87      | 17.13      | 0.1489       | 8.10       | OK\n006386          | 78.88      | 21.12      | 0.1580       | 8.08       | OK\n000920          | 78.11      | 21.89      | 0.1202       | 8.03       | OK\n000585          | 82.68      | 17.32      | 0.1552       | 8.29       | OK\n001593          | 86.23      | 13.77      | 0.1244       | 8.11       | OK\n004178          | 74.54      | 25.46      | 0.1641       | 8.03       | OK\n002011          | 61.27      | 38.73      | 0.1401       | 8.18       | OK\n007203          | 78.08      | 21.92      | 0.1286       | 8.28       | OK\n004710          | 93.35      | 6.65       | 0.1013       | 8.61       | OK\n002312          | 77.76      | 22.24      | 0.2872       | 8.57       | OK\n003141          | 81.53      | 18.47      | 0.1542       | 8.35       | OK\n003053          | 91.21      | 8.79       | 0.1567       | 8.01       | OK\n006833          | 80.01      | 19.99      | 0.1452       | 8.19       | OK\n004278          | 88.83      | 11.17      | 0.1859       | 8.66       | OK\n005398          | 83.45      | 16.55      | 0.1562       | 8.04       | OK\n000313          | 72.34      | 27.66      | 0.1724       | 8.31       | OK\n002084          | 80.61      | 19.39      | 0.1708       | 8.50       | OK\n000603          | 81.44      | 18.56      | 0.1498       | 8.54       | OK\n007284          | 76.80      | 23.20      | 0.1676       | 8.63       | OK\n001669          | 88.08      | 11.92      | 0.1236       | 8.39       | OK\n004656          | 96.06      | 3.94       | 0.1090       | 8.25       | OK\n007141          | 82.13      | 17.87      | 0.1824       | 8.60       | OK\n005328          | 80.94      | 19.06      | 0.1658       | 8.92       | OK\n003381          | 82.96      | 17.04      | 0.1548       | 8.50       | OK\n004772          | 82.76      | 17.24      | 0.1510       | 8.23       | OK\n005847          | 86.47      | 13.53      | 0.1728       | 8.74       | OK\n005504          | 58.79      | 41.21      | 0.1524       | 8.52       | OK\n005092          | 89.41      | 10.59      | 0.1565       | 8.71       | OK\n---------------------------------------------------------------------------------------------------------\nüéâ Ho√†n th√†nh: 50/50\n\nüìä TH·ªêNG K√ä TRUNG B√åNH (D√πng cho b√°o c√°o):\n   - Avg Foreground: 81.53%\n   - Avg Background: 18.47%\n   - Avg Depth:      0.1507\n   - Avg Time:       8.05 s/·∫£nh\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\n# T·∫°o th∆∞ m·ª•c ch·ª©a code\nos.makedirs(\"src/featglac\", exist_ok=True)\n\n# N·ªôi dung file code (ƒê√£ th√™m PATCH FIX cho PyTorch 2.1.2)\ninverter_code = r'''\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom typing import Optional, Union, Tuple, List\n\n# --- üî• HOTFIX: V√Å L·ªñI COMPATIBILITY PYTORCH 2.1.2 ---\n# L·ªói b·∫°n g·∫∑p l√† do accelerate g·ªçi h√†m c≈© 'grad_and_value' kh√¥ng c√≤n trong torch 2.1\n# Ta s·∫Ω ƒë·ªãnh nghƒ©a th·ªß c√¥ng n√≥ tr·ªè v·ªÅ 'torch.func.grad_and_value' (API m·ªõi)\ntry:\n    import torch._functorch.eager_transforms\n    if not hasattr(torch._functorch.eager_transforms, \"grad_and_value\"):\n        print(\"üõ†Ô∏è ƒêang √°p d·ª•ng b·∫£n v√°: torch._functorch.eager_transforms.grad_and_value\")\n        torch._functorch.eager_transforms.grad_and_value = torch.func.grad_and_value\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è C·∫£nh b√°o Patching: {e}\")\n# -----------------------------------------------------\n\nclass StableNullInverter:\n    def __init__(self, pipe):\n        self.pipe = pipe\n        self.tokenizer = pipe.tokenizer\n        self.text_encoder = pipe.text_encoder\n        self.vae = pipe.vae\n        self.unet = pipe.unet\n        self.scheduler = pipe.scheduler\n        self.device = pipe.device\n\n    @torch.no_grad()\n    def image2latent(self, image: Image.Image) -> torch.Tensor:\n        \"\"\"Chuy·ªÉn ·∫£nh PIL RGB -> Latent Tensor\"\"\"\n        # Resize v·ªÅ 512x512\n        image = image.resize((512, 512), resample=Image.Resampling.LANCZOS)\n        \n        # Normalize [-1, 1]\n        img_np = np.array(image).astype(np.float32) / 127.5 - 1.0\n        \n        # To Tensor [B, C, H, W]\n        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0)\n        img_tensor = img_tensor.to(self.device).to(self.pipe.dtype)\n        \n        # Encode VAE\n        latents = self.vae.encode(img_tensor).latent_dist.mean\n        latents = latents * 0.18215\n        return latents\n\n    @torch.no_grad()\n    def invert_input_image(\n        self, \n        image: Image.Image, \n        prompt: str = \"\", \n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.0\n    ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"Th·ª±c hi·ªán DDIM Inversion\"\"\"\n        \n        # 1. Encode ·∫£nh g·ªëc\n        latents = self.image2latent(image)\n        start_latents = latents.clone()\n        \n        # 2. Text Embeddings\n        text_input = self.tokenizer(\n            [prompt], \n            padding=\"max_length\", \n            max_length=self.tokenizer.model_max_length, \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        # 3. Scheduler Setup\n        self.scheduler.set_timesteps(num_inference_steps)\n        reversed_timesteps = self.scheduler.timesteps\n        if reversed_timesteps[0] > reversed_timesteps[-1]:\n            reversed_timesteps = torch.flip(reversed_timesteps, [0])\n\n        print(f\"üîÑ ƒêang Invert ({num_inference_steps} steps)...\")\n        \n        # L∆∞u qu·ªπ ƒë·∫°o (CPU ƒë·ªÉ ti·∫øt ki·ªám VRAM)\n        ddim_latents_all = [latents.cpu()] \n\n        # 4. Inversion Loop\n        for i, t in enumerate(tqdm(reversed_timesteps)):\n            noise_pred = self.unet(\n                latents, \n                t, \n                encoder_hidden_states=text_embeddings\n            ).sample\n\n            alpha_prod_t = self.scheduler.alphas_cumprod[t]\n            beta_prod_t = 1 - alpha_prod_t\n            \n            if i < len(reversed_timesteps) - 1:\n                next_t = reversed_timesteps[i+1]\n                alpha_prod_t_next = self.scheduler.alphas_cumprod[next_t]\n            else:\n                alpha_prod_t_next = self.scheduler.alphas_cumprod[0] \n\n            # DDIM Formula\n            pred_original_sample = (latents - beta_prod_t ** 0.5 * noise_pred) / (alpha_prod_t ** 0.5)\n            latents = alpha_prod_t_next ** 0.5 * pred_original_sample + (1 - alpha_prod_t_next) ** 0.5 * noise_pred\n            \n            ddim_latents_all.append(latents.cpu())\n\n        print(\"‚úÖ Inversion ho√†n t·∫•t.\")\n        return start_latents, latents, ddim_latents_all\n'''\n\nwith open(\"src/featglac/stable_null_inverter.py\", \"w\") as f:\n    f.write(inverter_code)\nprint(\"‚úÖ ƒê√£ t·∫°o file th∆∞ vi·ªán (K√®m b·∫£n v√° l·ªói PyTorch 2.1).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title üöë EMERGENCY PATCH: B·ªè qua l·ªói FSDP & Torch Distributed\nimport sys\nfrom unittest.mock import MagicMock\nimport torch\n\nprint(\"üöë ƒêANG V√Å L·ªñI XUNG ƒê·ªòT PYTORCH & ACCELERATE...\")\n\n# 1. Ch·∫∑n import module g√¢y l·ªói trong Torch\n# L·ªói '_fused_rms_norm_backward' n·∫±m trong torch.distributed.tensor._ops\n# Ta s·∫Ω thay th·∫ø to√†n b·ªô module n√†y b·∫±ng Mock (ƒë·ªì gi·∫£) ƒë·ªÉ Python kh√¥ng ch·∫°y v√†o ƒë√≥.\nsys.modules[\"torch.distributed.tensor._ops\"] = MagicMock()\nsys.modules[\"torch.distributed.tensor\"] = MagicMock()\nsys.modules[\"torch.distributed.checkpoint\"] = MagicMock()\nsys.modules[\"torch.distributed.fsdp\"] = MagicMock()\n\n# 2. Ch·∫∑n Accelerate g·ªçi FSDP Utils\nsys.modules[\"accelerate.utils.fsdp_utils\"] = MagicMock()\n\n# 3. V√° l·∫°i c√°c l·ªói c≈© (n·∫øu ch∆∞a ch·∫°y)\ntry:\n    if not hasattr(torch.ops.aten, \"_fused_rms_norm_backward\"):\n        # T·∫°o dummy attribute ƒë·ªÉ n·∫øu c√≥ module n√†o l·ªçt l∆∞·ªõi ki·ªÉm tra th√¨ kh√¥ng b·ªã crash\n        setattr(torch.ops.aten, \"_fused_rms_norm_backward\", MagicMock())\nexcept:\n    pass\n\n# 4. Patch Grad_and_value (cho Inversion)\ntry:\n    import torch._functorch.eager_transforms\n    if not hasattr(torch._functorch.eager_transforms, \"grad_and_value\"):\n        torch._functorch.eager_transforms.grad_and_value = torch.func.grad_and_value\nexcept: \n    pass\n\nprint(\"‚úÖ ƒê√É V√Å XONG! B·∫†N C√ì TH·ªÇ CH·∫†Y TI·∫æP CELL B√äN D∆Ø·ªöI.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title 7Ô∏è‚É£ RUN MODULE 2 (FIXED PATH): S·ª≠a l·ªói thi·∫øu StableNullInverter\nimport os\nimport sys\nimport gc\nimport glob\nimport time\nimport shutil\nimport types\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom unittest.mock import MagicMock\nfrom pyspark.sql import SparkSession\n\n# ================= C·∫§U H√åNH MODULE 2 =================\nINPUT_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module1\"\nOUTPUT_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module2\" \nNUM_INFERENCE_STEPS = 20\nSPARK_MASTER = \"local[1]\" \n\n# ================= üöë EMERGENCY PATCH: FIX L·ªñI TORCH.MTIA =================\nif not hasattr(torch, 'mtia'):\n    torch.mtia = types.ModuleType('mtia')\nif not hasattr(torch.mtia, '_set_stream_by_id'):\n    torch.mtia._set_stream_by_id = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'set_stream'):\n    torch.mtia.set_stream = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'synchronize'):\n    torch.mtia.synchronize = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'device'):\n    class FakeDevice: pass\n    torch.mtia.device = FakeDevice\n\n# ================= H√ÄM H·ªñ TR·ª¢ DRIVER =================\ndef clean_memory():\n    cleanup_vars = ['pipe', 'inverter', 'zoe_model', 'unet', 'vae']\n    for var in cleanup_vars:\n        if var in globals(): del globals()[var]\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ================= LOGIC WORKER (C√ì FIX PATH) =================\ndef inversion_worker(iterator):\n    import os\n    import sys\n    import torch\n    import time\n    import numpy as np\n    from PIL import Image\n    from diffusers import StableDiffusionPipeline \n    \n    # --- üî• FIX QUAN TR·ªåNG: √âP ƒê∆Ø·ªúNG D·∫™N CODE ---\n    base_dir = \"/kaggle/working/Depth-Aware-Editing\"\n    \n    # N·∫°p c·∫£ th∆∞ m·ª•c g·ªëc v√† th∆∞ m·ª•c src v√†o sys.path\n    paths_to_inject = [\n        base_dir,                       # ƒê·ªÉ import 'src.featglac...'\n        os.path.join(base_dir, \"src\")   # D·ª± ph√≤ng\n    ]\n    \n    for p in paths_to_inject:\n        if os.path.exists(p) and p not in sys.path:\n            sys.path.insert(0, p)\n    # ---------------------------------------------\n\n    try:\n        # Import class Inverter t·ª´ code d·ª± √°n\n        from src.featglac.stable_null_inverter import StableNullInverter\n    except ImportError as e:\n        # N·∫øu l·ªói, tr·∫£ v·ªÅ th√¥ng b√°o debug chi ti·∫øt\n        yield (\"INIT_ERROR\", f\"Missing StableNullInverter: {str(e)}\", 0, 0)\n        return\n\n    # 1. Load Model SD v1.5\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_id = \"runwayml/stable-diffusion-v1-5\"\n    \n    try:\n        pipe = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            safety_checker=None\n        ).to(device)\n        \n        # Kh·ªüi t·∫°o Inverter\n        inverter = StableNullInverter(pipe)\n    except Exception as e:\n        yield (\"MODEL_LOAD_ERROR\", str(e), 0, 0)\n        return\n\n    # 2. Loop x·ª≠ l√Ω ·∫£nh\n    for row in iterator:\n        img_id = row.img_id\n        img_path = row.img_path\n        target_output_dir = row.output_dir \n        \n        start_time = time.time()\n        \n        try:\n            # T·∫°o th∆∞ m·ª•c output ri√™ng cho ·∫£nh n√†y\n            save_folder = os.path.join(target_output_dir, img_id)\n            os.makedirs(save_folder, exist_ok=True)\n            \n            latent_path = os.path.join(save_folder, \"inverted_latents.pt\")\n            \n            # Check t·ªìn t·∫°i ƒë·ªÉ skip (Resume capability)\n            if os.path.exists(latent_path):\n                yield (img_id, \"SKIPPED\", 0, 0)\n                continue\n\n            # Load ·∫£nh input t·ª´ Module 1\n            input_image = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n            \n            # --- TH·ª∞C HI·ªÜN INVERSION ---\n            with torch.autocast(\"cuda\"):\n                # H√†m invert tr·∫£ v·ªÅ (start_latents, inverted_latents, all_latents)\n                # Ta c·∫ßn l∆∞u start_latents (z_T) ƒë·ªÉ d√πng cho Module 3\n                _, inverted_latents, _ = inverter.invert_input_image(\n                    input_image, \n                    prompt=\"\", \n                    num_inference_steps=NUM_INFERENCE_STEPS\n                )\n            \n            # L∆∞u file k·∫øt qu·∫£ .pt\n            torch.save(inverted_latents.cpu(), latent_path)\n            \n            # --- T√çNH TO√ÅN SAI S·ªê (MSE) ---\n            # Reconstruct ·∫£nh ƒë·ªÉ ƒëo ƒë·ªô sai l·ªách\n            with torch.no_grad():\n                z0 = inverter.image2latent(input_image)\n                recon_img_tensor = pipe.vae.decode(z0 / 0.18215).sample\n                recon_img = (recon_img_tensor / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).numpy()[0]\n                \n            orig_img = np.array(input_image).astype(np.float32) / 255.0\n            mse = np.mean((orig_img - recon_img) ** 2)\n            \n            process_time = time.time() - start_time\n            yield (img_id, \"SUCCESS\", round(process_time, 2), round(mse, 6))\n            \n        except Exception as e:\n            yield (img_id, f\"ERROR: {str(e)}\", 0, 0)\n\n# ================= MAIN DRIVER =================\ndef main_module2():\n    clean_memory()\n    print(f\"üöÄ Kh·ªüi ch·∫°y Spark Module 2 (Inversion) - Output: {OUTPUT_DIR}\")\n    \n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    spark = SparkSession.builder \\\n        .appName(\"Module2_Inversion_Fixed\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .config(\"spark.executor.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    # 1. Qu√©t file input t·ª´ Module 1 (File 'input.png' do Module 1 t·∫°o ra)\n    # C·∫•u tr√∫c Module 1: spark_output/module1/{img_id}/input.png\n    search_pattern = os.path.join(INPUT_DIR, \"*\", \"input.png\")\n    input_files = glob.glob(search_pattern)\n    \n    if not input_files:\n        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh input n√†o t·∫°i: {INPUT_DIR}\")\n        print(\"üëâ H√£y ch·∫Øc ch·∫Øn Module 1 ƒë√£ ch·∫°y th√†nh c√¥ng v√† t·∫°o file input.png\")\n        spark.stop()\n        return\n        \n    print(f\"üìÇ T√¨m th·∫•y {len(input_files)} ·∫£nh ƒë·∫ßu v√†o t·ª´ Module 1.\")\n    \n    # T·∫°o DataFrame\n    data = []\n    for f in input_files:\n        img_id = os.path.basename(os.path.dirname(f))\n        data.append((img_id, f, OUTPUT_DIR))\n    \n    df = spark.createDataFrame(data, [\"img_id\", \"img_path\", \"output_dir\"])\n    \n    # 2. Ch·∫°y tr√™n Spark\n    df_part = df.repartition(1) \n    print(\"üîÑ ƒêang th·ª±c hi·ªán Inversion (Qu√° tr√¨nh n√†y t·ªën GPU)...\")\n    \n    results_rdd = df_part.rdd.mapPartitions(inversion_worker)\n    final_results = results_rdd.collect()\n    \n    # 3. Hi·ªÉn th·ªã k·∫øt qu·∫£\n    print(\"\\n‚úÖ K·∫æT QU·∫¢ MODULE 2:\")\n    print(f\"{'IMAGE ID':<15} | {'STATUS':<15} | {'TIME (s)':<10} | {'MSE (Loss)'}\")\n    print(\"-\" * 65)\n    \n    success_count = 0\n    stats_data = []\n    \n    for img_id, status, p_time, mse in final_results:\n        print(f\"{img_id[:15]:<15} | {status[:15]:<15} | {p_time:<10} | {mse}\")\n        if status == \"SUCCESS\":\n            success_count += 1\n            stats_data.append({\"Image ID\": img_id, \"Time\": p_time, \"MSE\": mse})\n            \n    if stats_data:\n        pd.DataFrame(stats_data).to_csv(os.path.join(OUTPUT_DIR, \"module2_stats.csv\"), index=False)\n        \n    print(\"-\" * 65)\n    print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(final_results)} ·∫£nh.\")\n    \n    spark.stop()\n    clean_memory()\n\nif __name__ == \"__main__\":\n    main_module2()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title 8Ô∏è‚É£ RUN MODULE 3 (SPARK - FEATURE EXTRACTION): Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h√†ng lo·∫°t\nimport os\nimport sys\nimport gc\nimport glob\nimport time\nimport shutil\nimport types\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom unittest.mock import MagicMock\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\n\n# ================= C·∫§U H√åNH MODULE 3 =================\n# Input l·∫•y t·ª´ output c·ªßa Module 2\nINPUT_DIR_M2 = \"/kaggle/working/Depth-Aware-Editing/spark_output/module2\"\nOUTPUT_DIR_M3 = \"/kaggle/working/Depth-Aware-Editing/spark_output/module3\"\nSPARK_MASTER = \"local[1]\"\n\n# ================= üöë EMERGENCY PATCH: TORCH.MTIA =================\nif not hasattr(torch, 'mtia'):\n    torch.mtia = types.ModuleType('mtia')\nif not hasattr(torch.mtia, '_set_stream_by_id'):\n    torch.mtia._set_stream_by_id = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'set_stream'):\n    torch.mtia.set_stream = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'synchronize'):\n    torch.mtia.synchronize = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'device'):\n    class FakeDevice: pass\n    torch.mtia.device = FakeDevice\n\n# ================= üõ°Ô∏è SYSTEM PATCH V4 (DIFFUSERS) =================\nsys.modules[\"torch.onnx\"] = MagicMock()\nif \"diffusers.hooks\" not in sys.modules:\n    hooks_pkg = types.ModuleType(\"diffusers.hooks\")\n    hooks_pkg.__path__ = [] \n    sys.modules[\"diffusers.hooks\"] = hooks_pkg\n\nif \"diffusers.hooks.group_offloading\" not in sys.modules:\n    offload_mod = types.ModuleType(\"diffusers.hooks.group_offloading\")\n    sys.modules[\"diffusers.hooks.group_offloading\"] = offload_mod\nelse:\n    offload_mod = sys.modules[\"diffusers.hooks.group_offloading\"]\n\nsys.modules[\"diffusers.hooks\"].group_offloading = offload_mod\ndef _fake_get_group_onload_device(*args, **kwargs): return torch.device(\"cpu\")\ndef _fake_is_group_offload_enabled(*args, **kwargs): return False\noffload_mod._get_group_onload_device = _fake_get_group_onload_device\noffload_mod._is_group_offload_enabled = _fake_is_group_offload_enabled\n\ntry:\n    import diffusers.models.model_loading_utils\n    def _bypass_check(*args, **kwargs): return None\n    diffusers.models.model_loading_utils.check_support_param_buffer_assignment = _bypass_check\nexcept ImportError: pass\n\n# ================= CLASS: FEATURE EXTRACTOR =================\nclass FeatureExtractor:\n    def __init__(self, unet):\n        self.unet = unet\n        self.features = {}\n        self.hooks = []\n        \n        # Hook v√†o c√°c t·∫ßng Up-block (Decoder)\n        for i, block in enumerate(unet.up_blocks):\n            layer_name = f\"up_block_{i}\"\n            hook = block.register_forward_hook(self._get_hook_fn(layer_name))\n            self.hooks.append(hook)\n            \n        if hasattr(unet, \"mid_block\") and unet.mid_block is not None:\n             hook = unet.mid_block.register_forward_hook(self._get_hook_fn(\"mid_block\"))\n             self.hooks.append(hook)\n\n    def _get_hook_fn(self, layer_name):\n        def hook(module, input, output):\n            if isinstance(output, tuple): feature_map = output[0]\n            else: feature_map = output\n            self.features[layer_name] = feature_map.detach().cpu()\n        return hook\n\n    def remove_hooks(self):\n        for hook in self.hooks: hook.remove()\n        self.hooks = []\n\n# ================= LOGIC WORKER (SPARK) =================\ndef feature_worker(iterator):\n    import torch\n    import os\n    import numpy as np\n    from diffusers import StableDiffusionPipeline, DDIMScheduler\n\n    # Load Model (1 l·∫ßn cho c·∫£ partition)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n    \n    try:\n        scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n        pipe = StableDiffusionPipeline.from_pretrained(\n            MODEL_ID, scheduler=scheduler, torch_dtype=torch.float32, # Feature extraction c·∫ßn ƒë·ªô ch√≠nh x√°c cao\n            low_cpu_mem_usage=False, device_map=None, use_safetensors=True\n        ).to(device)\n        pipe.set_progress_bar_config(disable=True)\n    except Exception as e:\n        yield (\"INIT_ERROR\", str(e), \"\", 0, 0, 0, 0)\n        return\n\n    # Loop x·ª≠ l√Ω t·ª´ng ·∫£nh\n    for row in iterator:\n        img_id = row.img_id\n        latent_path = row.latent_path\n        \n        try:\n            # 1. Load Latent\n            # File t·ª´ Module 2 l√† 'inverted_latents.pt'\n            latents = torch.load(latent_path).to(device, dtype=torch.float32)\n\n            # 2. Setup Extractor\n            extractor = FeatureExtractor(pipe.unet)\n\n            # 3. Forward Pass (Ch·∫°y qua UNet ƒë·ªÉ k√≠ch ho·∫°t features)\n            t = torch.tensor([0]).to(device) # timestep 0\n            with torch.no_grad():\n                prompt_embeds, _ = pipe.encode_prompt(\n                    prompt=\"\", device=device, num_images_per_prompt=1, do_classifier_free_guidance=False\n                )\n                prompt_embeds = prompt_embeds.to(dtype=torch.float32)\n                pipe.unet(latents, t, encoder_hidden_states=prompt_embeds)\n            \n            # 4. L∆∞u Features & T√≠nh Stats\n            save_dir = os.path.join(OUTPUT_DIR_M3, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            \n            # L∆∞u stats c·ªßa layer quan tr·ªçng nh·∫•t: 'up_block_2' (th∆∞·ªùng ch·ª©a semantic t·ªët nh·∫•t)\n            target_layer = \"up_block_2\"\n            stats_tuple = (0, 0, 0, 0)\n            \n            for name, tensor in extractor.features.items():\n                # L∆∞u file .pt\n                torch.save(tensor, os.path.join(save_dir, f\"{name}.pt\"))\n                \n                # T√≠nh stats n·∫øu l√† layer m·ª•c ti√™u\n                if name == target_layer:\n                    arr = tensor.numpy()\n                    stats_tuple = (\n                        float(np.mean(arr)),\n                        float(np.std(arr)),\n                        float(np.min(arr)),\n                        float(np.max(arr))\n                    )\n            \n            # V·∫Ω heatmap ƒë·∫°i di·ªán (l∆∞u ·∫£nh png)\n            if target_layer in extractor.features:\n                feat = extractor.features[target_layer]\n                heatmap = feat[0].mean(dim=0).float().numpy()\n                plt.figure(figsize=(4, 4))\n                plt.imshow(heatmap, cmap='magma')\n                plt.axis('off')\n                plt.tight_layout()\n                plt.savefig(os.path.join(save_dir, \"heatmap_viz.png\"))\n                plt.close()\n\n            extractor.remove_hooks()\n            \n            yield (img_id, \"SUCCESS\", target_layer, *stats_tuple)\n            \n        except Exception as e:\n            yield (img_id, f\"ERROR: {str(e)}\", \"\", 0, 0, 0, 0)\n\n# ================= SPARK DRIVER =================\ndef main_module3():\n    # D·ªçn d·∫πp\n    if 'spark' in globals(): globals()['spark'].stop()\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    print(f\"üöÄ Kh·ªüi ch·∫°y Spark Module 3 (Feature Extraction)\")\n    print(f\"üìÇ Input: {INPUT_DIR_M2}\")\n    print(f\"üìÇ Output: {OUTPUT_DIR_M3}\")\n    \n    os.makedirs(OUTPUT_DIR_M3, exist_ok=True)\n    \n    spark = SparkSession.builder \\\n        .appName(\"Module3_Features\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n\n    # 1. Qu√©t d·ªØ li·ªáu t·ª´ Module 2\n    # C·∫•u tr√∫c: module2/{img_id}/inverted_latents.pt\n    search_pattern = os.path.join(INPUT_DIR_M2, \"*\", \"inverted_latents.pt\")\n    latent_files = glob.glob(search_pattern)\n    \n    if not latent_files:\n        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file 'inverted_latents.pt' n√†o trong {INPUT_DIR_M2}\")\n        spark.stop()\n        return\n    \n    print(f\"‚úÖ T√¨m th·∫•y {len(latent_files)} b·ªô Latent ƒë·ªÉ x·ª≠ l√Ω.\")\n    \n    # 2. T·∫°o DataFrame\n    data = []\n    for f in latent_files:\n        img_id = os.path.basename(os.path.dirname(f))\n        data.append((img_id, f))\n        \n    df = spark.createDataFrame(data, [\"img_id\", \"latent_path\"])\n    df_part = df.repartition(1)\n    \n    # 3. Ch·∫°y Worker\n    print(\"üîÑ ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (UNet Forward Pass)...\")\n    results_rdd = df_part.rdd.mapPartitions(feature_worker)\n    \n    # ƒê·ªãnh nghƒ©a Schema cho k·∫øt qu·∫£\n    schema = StructType([\n        StructField(\"Image_ID\", StringType(), True),\n        StructField(\"Status\", StringType(), True),\n        StructField(\"Layer\", StringType(), True),\n        StructField(\"Mean\", FloatType(), True),\n        StructField(\"Std\", FloatType(), True),\n        StructField(\"Min\", FloatType(), True),\n        StructField(\"Max\", FloatType(), True)\n    ])\n    \n    # Collect v·ªÅ DataFrame ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp\n    try:\n        final_df = spark.createDataFrame(results_rdd, schema=schema)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä B·∫¢NG TH·ªêNG K√ä FEATURE MAPS (L·ªõp: up_block_2)\")\n        print(\"=\"*80)\n        final_df.show(truncate=False)\n        \n        # L∆∞u b·∫£ng th·ªëng k√™\n        final_df.toPandas().to_csv(os.path.join(OUTPUT_DIR_M3, \"feature_stats.csv\"), index=False)\n        \n        # ƒê·∫øm th√†nh c√¥ng\n        success_count = final_df.filter(\"Status = 'SUCCESS'\").count()\n        print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(latent_files)} ·∫£nh.\")\n        \n    except Exception as e:\n        print(f\"‚ùå L·ªói khi hi·ªÉn th·ªã b·∫£ng: {e}\")\n        # In raw n·∫øu l·ªói DataFrame\n        print(results_rdd.collect())\n\n    spark.stop()\n\nif __name__ == \"__main__\":\n    main_module3()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL \"C·∫§P C·ª®U\": GI·∫¢I PH√ìNG RAM CPU V√Ä GPU TRI·ªÜT ƒê·ªÇ\n# ==============================================================================\nimport torch\nimport gc\nimport ctypes\nimport sys\n\nprint(\"üßπ ƒêang ti·∫øn h√†nh t·ªïng v·ªá sinh b·ªô nh·ªõ (CPU & GPU)...\")\n\n# 1. X√≥a c√°c bi·∫øn l·ªõn trong Python\n# Danh s√°ch c√°c t√™n bi·∫øn th∆∞·ªùng d√πng trong notebook n√†y\ntargets = ['model', 'diff_handles', 'activations', 'loaded_feats', \n           'init_noise', 'depth_tensor', 'mpi_masks', 'results', \n           'sd', 'state_dict', 'checkpoint']\n\nfor t in targets:\n    if t in globals():\n        del globals()[t]\n        print(f\"   - ƒê√£ x√≥a bi·∫øn Python: {t}\")\n\n# 2. √âp Garbage Collector ch·∫°y\ngc.collect()\n\n# 3. QUAN TR·ªåNG: √âp Linux thu h·ªìi b·ªô nh·ªõ RAM (Malloc Trim)\n# L·ªánh n√†y gi√∫p RAM gi·∫£m ngay l·∫≠p t·ª©c tr√™n Kaggle/Colab\ntry:\n    libc = ctypes.CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\n    print(\"   - ƒê√£ ch·∫°y l·ªánh h·ªá th·ªëng: libc.malloc_trim(0)\")\nexcept Exception as e:\n    print(f\"   - Kh√¥ng g·ªçi ƒë∆∞·ª£c libc (Kh√¥ng sao): {e}\")\n\n# 4. X·∫£ Cache GPU\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    print(\"   - ƒê√£ x·∫£ CUDA Cache.\")\n\nprint(\"-\" * 30)\nprint(\"‚úÖ HO√ÄN T·∫§T! H√ÉY KI·ªÇM TRA L·∫†I THANH RAM ·ªû G√ìC TR√äN.\")\nprint(\"üëâ Sau ƒë√≥ ch·∫°y l·∫°i Cell Load Model b√™n d∆∞·ªõi.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# B∆Ø·ªöC 2: LOAD MODEL SD 1.5 INPAINTING (B·∫¢N V√Å V11 - FIX INPAINTING CONFIG)\n# ==============================================================================\nimport torch\nimport os\nimport sys\nimport requests\nimport glob\nimport gc\nimport ctypes\nimport types\nimport yaml\nfrom unittest.mock import MagicMock\n\nprint(\"üöë ƒêANG K√çCH HO·∫†T H·ªÜ TH·ªêNG V√Å L·ªñI V11 (CHANNEL MISMATCH FIX)...\")\n\n# --- 1. S·ª¨A FILE CONFIG T·ª™ 4 K√äNH -> 9 K√äNH (L·ªñI C·ª¶A B·∫†N) ---\n# T·∫£i config chu·∫©n tr∆∞·ªõc\nconfig_path = \"cldm_v15_fixed.yaml\"\nif not os.path.exists(config_path):\n    url = \"https://raw.githubusercontent.com/lllyasviel/ControlNet/main/models/cldm_v15.yaml\"\n    try:\n        with open(config_path, \"wb\") as f: f.write(requests.get(url).content)\n    except: pass\n\n# ƒê·ªçc v√† s·ª≠a config\ntry:\n    with open(config_path, 'r') as f:\n        config_data = yaml.safe_load(f)\n    \n    # Ki·ªÉm tra v√† s·ª≠a in_channels\n    current_channels = config_data['model']['params']['unet_config']['params']['in_channels']\n    print(f\"   ‚ÑπÔ∏è Config g·ªëc c√≥ {current_channels} k√™nh ƒë·∫ßu v√†o.\")\n    \n    if current_channels != 9:\n        print(\"   üîß ƒêang chuy·ªÉn ƒë·ªïi Config sang ch·∫ø ƒë·ªô Inpainting (9 k√™nh)...\")\n        config_data['model']['params']['unet_config']['params']['in_channels'] = 9\n        \n        # L∆∞u ra file m·ªõi\n        new_config_path = \"cldm_v15_inpainting_patched.yaml\"\n        with open(new_config_path, 'w') as f:\n            yaml.dump(config_data, f)\n        config_path = new_config_path # Tr·ªè sang file m·ªõi\n        print(f\"   ‚úÖ ƒê√£ t·∫°o config m·ªõi: {config_path}\")\n    else:\n        print(\"   ‚úÖ Config ƒë√£ chu·∫©n 9 k√™nh.\")\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è L·ªói khi s·ª≠a config: {e}\")\n\n# --- 2. FIX C√ÅC L·ªñI KH√ÅC (GI·ªÆ NGUY√äN T·ª™ V10) ---\n# Fix missing anydoor config\nrequired_config_path = \"/kaggle/working/configs/anydoor.yaml\"\nif not os.path.exists(required_config_path):\n    os.makedirs(os.path.dirname(required_config_path), exist_ok=True)\n    dummy_content = \"\"\"\nmodel:\n  target: ldm.models.diffusion.ddpm.LatentDiffusion\n  params:\n    linear_start: 0.00085\n    linear_end: 0.0120\n    num_timesteps_cond: 1\n    log_every_t: 200\n    timesteps: 1000\n    first_stage_key: \"jpg\"\n    cond_stage_key: \"txt\"\n    image_size: 64\n    channels: 4\n    cond_stage_trainable: false\n    conditioning_key: crossattn\n    monitor: val/loss_simple_ema\n    scale_factor: 0.18215\n    use_ema: False\n    cond_stage_config:\n      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder\n      params:\n        freeze: True\n        layer: \"penultimate\"\n      weight: \"weights/dinov2_vitg14_pretrain.pth\"\n\"\"\"\n    with open(required_config_path, \"w\") as f: f.write(dummy_content)\n\n# Patch ONNX & FSDP\nfake_onnx = types.ModuleType(\"torch.onnx\")\nfake_onnx.__path__ = []\nfake_onnx.symbolic_helper = MagicMock()\nfake_onnx.utils = MagicMock()\nfake_onnx.symbolic_opset11 = MagicMock()\nfake_onnx.symbolic_opset9 = MagicMock()\nfake_onnx.symbolic_opset10 = MagicMock()\nfake_onnx.register_custom_op_symbolic = MagicMock(return_value=None)\nfake_onnx.unregister_custom_op_symbolic = MagicMock(return_value=None)\nfake_onnx.select_model_mode_for_export = MagicMock(return_value=None)\n\nsys.modules[\"torch.onnx\"] = fake_onnx\nsys.modules[\"torch.onnx.symbolic_helper\"] = fake_onnx.symbolic_helper\nsys.modules[\"torch.onnx.utils\"] = fake_onnx.utils\nsys.modules[\"torch.onnx.symbolic_opset11\"] = fake_onnx.symbolic_opset11\nfake_onnx.symbolic_helper.parse_args = MagicMock(return_value=lambda x: x)\nfake_onnx.symbolic_helper._parse_arg = MagicMock(return_value=lambda x: x)\n\nclass MockClass:\n    def __init__(self, *args, **kwargs): pass\n\nfake_fsdp = types.ModuleType(\"torch.distributed.fsdp\")\nfake_fsdp.__path__ = []\nfake_fsdp.CPUOffload = MockClass\nfake_fsdp.FullyShardedDataParallel = MockClass\nfake_fsdp.MixedPrecision = MockClass\nfake_fsdp.BackwardPrefetch = MockClass\nfake_fsdp.ShardingStrategy = MockClass\nfake_fsdp.StateDictType = MockClass\nfake_fsdp.FullStateDictConfig = MockClass\n\nsys.modules[\"torch.distributed.fsdp\"] = fake_fsdp\nsys.modules[\"torch.distributed.fsdp.fully_sharded_data_parallel\"] = fake_fsdp\nsys.modules[\"torch.distributed.fsdp.sharded_grad_scaler\"] = MagicMock()\nsys.modules[\"torch.distributed.fsdp.wrap\"] = MagicMock()\n\nfake_algo = types.ModuleType(\"torch.distributed.algorithms\")\nsys.modules[\"torch.distributed.algorithms\"] = fake_algo\nsys.modules[\"torch.distributed.algorithms.model_averaging\"] = MagicMock()\nsys.modules[\"torch.distributed.algorithms.model_averaging.averagers\"] = MagicMock()\n\nprint(\"   ‚úÖ [2/2] ƒê√£ v√° Environment & Config.\")\nprint(\"üöÄ TI·∫æP T·ª§C LOAD MODEL...\")\n# -----------------------------------------------------------\n\n# Setup path\nsys.path.append(os.getcwd())\nif os.path.exists(\"/kaggle/working/Depth-Aware-Editing\"):\n    sys.path.append(\"/kaggle/working/Depth-Aware-Editing\")\n\ntry:\n    from cldm.model import create_model, load_state_dict\n    from cldm.ddim_hacked_mpi_featguidance import DDIMSampler\nexcept ImportError:\n    if os.path.exists(\"/kaggle/temp/Depth-Aware-Editing\"):\n        os.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n        sys.path.append(\"/kaggle/temp/Depth-Aware-Editing\")\n        from cldm.model import create_model, load_state_dict\n        from cldm.ddim_hacked_mpi_featguidance import DDIMSampler\n    else:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c source code!\")\n\n# Ch·ªçn GPU 0\ntarget_device = \"cuda:0\"\nckpt_path = '/kaggle/working/Depth-Aware-Editing/weights/sd-v1-5-inpainting.ckpt'\n\n# S·ª≠ d·ª•ng config_path ƒë√£ ƒë∆∞·ª£c v√° ·ªü tr√™n\nprint(f\"‚ÑπÔ∏è S·ª≠ d·ª•ng Config: {config_path}\")\n\ntry:\n    print(\"üì¶ T·∫°o khung Model (CPU)...\")\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    \n    model = create_model(config_path).cpu()\n    \n    print(\"üìÇ ƒêang ƒë·ªçc checkpoint...\")\n    if not os.path.exists(ckpt_path):\n        found = glob.glob(\"/kaggle/working/**/sd-v1-5-inpainting.ckpt\", recursive=True)\n        if found: ckpt_path = found[0]\n        else: \n            print(\"‚¨áÔ∏è ƒêang t·∫£i weights d·ª± ph√≤ng...\")\n            os.system(f\"wget -q -O {ckpt_path} https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt\")\n            \n    if not os.path.exists(ckpt_path): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y file checkpoint!\")\n\n    # Load Weights\n    sd = load_state_dict(ckpt_path, location='cpu')\n    if 'state_dict' in sd: sd = sd['state_dict']\n    \n    print(\"‚è≥ ƒêang n·∫°p weights v√†o model (Strict=False)...\")\n    m, u = model.load_state_dict(sd, strict=False)\n    print(f\"   -> Model loaded. Missing keys: {len(m)}, Unexpected keys: {len(u)}\")\n    \n    # Clean RAM\n    del sd\n    gc.collect()\n    try: ctypes.CDLL(\"libc.so.6\").malloc_trim(0) \n    except: pass\n\n    # Move to GPU (FIX: D√πng FP32 thay v√¨ FP16)\n    print(f\"üöö ƒêang ƒë·∫©y model sang {target_device} (FP32)...\")\n    model = model.float().to(target_device)  # FIX: float() thay v√¨ half()\n    \n    diff_handles = DDIMSampler(model)\n    device = torch.device(target_device)\n    \n    print(\"=\"*60)\n    print(\"üéâ LOAD MODEL TH√ÄNH C√îNG!\")\n    print(f\"   - Checkpoint: {os.path.basename(ckpt_path)}\")\n    print(f\"   - VRAM Used: {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n    print(\"üëâ Model ƒë√£ s·∫µn s√†ng. H√£y ch·∫°y ti·∫øp Cell Module 4.\")\n    print(\"=\"*60)\n\nexcept Exception as e:\n    print(f\"‚ùå V·∫´n c√≤n l·ªói: {e}\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title üöÄ MODULE 4: Scene Composition (Simplified + Spark)\nimport os\nimport sys\nimport gc\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport torch.nn. functional as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql. types import StructType, StructField, StringType\nimport matplotlib. pyplot as plt\n\nprint(\"üöÄ MODULE 4: SIMPLIFIED SCENE COMPOSITION\")\n\n# ================= CONFIG =================\nBASE_WORK_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output\"\nDIR_M1 = os.path.join(BASE_WORK_DIR, \"module1\")\nDIR_M2 = os.path.join(BASE_WORK_DIR, \"module2\")\nDIR_M3 = os.path.join(BASE_WORK_DIR, \"module3\")\nOUTPUT_DIR_M4 = os.path.join(BASE_WORK_DIR, \"module4\")\nPROMPT = \"a high quality photo of a living room\"\nSPARK_MASTER = \"local[1]\"\n\ndef clean_memory():\n    for var in ['pipe', 'model']: \n        if var in globals(): del globals()[var]\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\nclean_memory()\n\n# ================= WORKER =================\ndef generation_worker(iterator):\n    import os, sys, torch, numpy as np\n    from PIL import Image\n    from scipy.ndimage import gaussian_filter\n    \n    sys.path.insert(0, \"/kaggle/working/Depth-Aware-Editing\")\n    \n    try:\n        from diffusers import StableDiffusionImg2ImgPipeline\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            safety_checker=None\n        ).to(device)\n        pipe.set_progress_bar_config(disable=True)\n    except Exception as e:\n        yield (\"INIT_ERROR\", str(e), \"\")\n        return\n    \n    for row in iterator:\n        img_id = row. img_id\n        try:\n            path_m1 = os.path.join(DIR_M1, img_id)\n            \n            # Load image & mask\n            input_img = Image.open(os.path.join(path_m1, \"input.png\")).convert(\"RGB\").resize((512, 512))\n            mask_pil = Image.open(os. path.join(path_m1, \"mask.png\")).convert(\"L\").resize((512, 512))\n            mask_np = np.array(mask_pil) / 255.0\n            \n            # Blur masked region\n            input_np = np.array(input_img)\n            blurred = gaussian_filter(input_np, sigma=[5, 5, 0])\n            mask_3ch = np.stack([mask_np]*3, axis=-1)\n            blended = input_np * (1 - mask_3ch) + blurred * mask_3ch\n            blended_img = Image.fromarray(blended.astype(np.uint8))\n            \n            # Generate\n            with torch.no_grad():\n                output = pipe(\n                    prompt=PROMPT,\n                    image=blended_img,\n                    strength=0.75,\n                    guidance_scale=7.5,\n                    num_inference_steps=50\n                ).images[0]\n            \n            # Save\n            save_dir = os.path.join(OUTPUT_DIR_M4, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            output_path = os.path.join(save_dir, \"final_result.png\")\n            output. save(output_path)\n            \n            # Save comparison\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n            axes[0].imshow(input_img); axes[0].set_title(\"Input\"); axes[0].axis('off')\n            axes[1].imshow(mask_pil, cmap='gray'); axes[1].set_title(\"Mask\"); axes[1].axis('off')\n            axes[2].imshow(output); axes[2].set_title(\"Output\"); axes[2].axis('off')\n            plt.tight_layout()\n            plt.savefig(os.path.join(save_dir, \"comparison.png\"), dpi=100)\n            plt.close()\n            \n            yield (img_id, \"SUCCESS\", output_path)\n            \n        except Exception as e: \n            yield (img_id, \"ERROR\", str(e)[:80])\n\n# ================= MAIN =================\ndef main():\n    if not os.path.exists(DIR_M1):\n        print(\"‚ùå Module 1 ch∆∞a ch·∫°y!\")\n        return\n    \n    img_ids = [d for d in os.listdir(DIR_M1) if os.path.isdir(os.path.join(DIR_M1, d))]\n    if not img_ids:\n        print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu\")\n        return\n    \n    print(f\" Found {len(img_ids)} images\")\n    os.makedirs(OUTPUT_DIR_M4, exist_ok=True)\n    \n    # Spark\n    spark = SparkSession.builder \\\n        .appName(\"Module4\") \\\n        .config(\"spark. driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    spark.sparkContext. setLogLevel(\"ERROR\")\n    \n    schema = StructType([StructField(\"img_id\", StringType(), True)])\n    df = spark.createDataFrame([(i,) for i in img_ids], schema=schema)\n    \n    print(\"\\n Processing...\")\n    results = df.rdd.mapPartitions(generation_worker).collect()\n    \n    # Results\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{'IMAGE ID':<20} | {'STATUS':<15} | {'OUTPUT'}\")\n    print(\"-\"*80)\n    \n    success = 0\n    for img_id, status, note in results:\n        display_note = note[-40: ] if len(note) > 40 else note\n        if status == \"SUCCESS\":\n            success += 1\n            print(f\"{img_id:<20} | {status:<13} | ... {display_note}\")\n        else:\n            print(f\"{img_id:<20} | {status:<13} | {display_note}\")\n    \n    print(\"-\"*80)\n    print(f\"Success: {success}/{len(results)}\")\n    print(f\"Output: {OUTPUT_DIR_M4}\")\n    print(\"=\"*80)\n    \n    spark.stop()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}